<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.6.3" />
<title>neurochat.nc_lfp API documentation</title>
<meta name="description" content="This module implements NLfp Class for NeuroChaT software …" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase;cursor:pointer}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>neurochat.nc_lfp</code></h1>
</header>
<section id="section-intro">
<p>This module implements NLfp Class for NeuroChaT software</p>
<p>@author: Md Nurul Islam; islammn at tcd dot ie</p>
<details class="source">
<summary>Source code</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
&#34;&#34;&#34;
This module implements NLfp Class for NeuroChaT software

@author: Md Nurul Islam; islammn at tcd dot ie
&#34;&#34;&#34;
import os

import re
import inspect
from functools import reduce

import logging
from collections import OrderedDict as oDict
from copy import deepcopy

from math import floor, ceil
from neurochat.nc_utils import window_rms
from neurochat.nc_utils import butter_filter
from neurochat.nc_utils import find_peaks

from neurochat.nc_utils import butter_filter, fft_psd, find

from neurochat.nc_circular import CircStat
from neurochat.nc_hdf import Nhdf
from neurochat.nc_base import NBase

import numpy as np


import scipy.stats as stats
import scipy.signal as sg
from scipy.fftpack import fft

class NLfp(NBase):
    &#34;&#34;&#34;
    This data class is the placeholder for the dataset that contains information
    about the neural LFP signal. It decodes data from different formats and analyses
    LFP signal in the recording.

    &#34;&#34;&#34;

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._file_tag = &#39;&#39;
        self._channel_id = 0
        self._samples = None
        self._timestamp = None
        self.set_record_info({&#39;Total samples&#39;: 0})

        self.__type = &#39;lfp&#39;

    def get_type(self):
        &#34;&#34;&#34;
        Returns the type of object. For NLfp, this is always `lfp` type

        Parameters
        ----------
        None

        Returns
        -------
        str

        &#34;&#34;&#34;
        return self.__type

    # For multi-unit analysis, {&#39;SpikeName&#39;: cell_no} pairs should be used as function input

    def set_channel_id(self, channel_id=&#39;&#39;):
        &#34;&#34;&#34;
        Sets the electrode channels ID

        Parameters
        ----------
        channel_id : str
            Channel ID for the LFP data

        Returns
        -------
        None

        &#34;&#34;&#34;
        self._channel_id = channel_id

    def get_channel_id(self):
        &#34;&#34;&#34;
        Returns the electrode channels ID

        Parameters
        ----------
        None

        Returns
        -------
        str
            LFP channel ID

        &#34;&#34;&#34;

        return self._channel_id

    def set_file_tag(self, file_tag):
        &#34;&#34;&#34;
        Sets the file tag or extension for the LFP dataset. For example, Axona recordings usually
        have file tags like &#39;eeg&#39; or &#39;eeg8&#39; etc.

        Parameters
        ----------
        file_tag : str
            File tag or extension for the LFP dataset

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._file_tag = file_tag

    def get_file_tag(self):
        &#34;&#34;&#34;
        Returns the file tag or extension for the LFP dataset. For example, Axona recordings usually
        have file tags like &#39;eeg&#39; or &#39;eeg8&#39; etc.

        Parameters
        ----------
        None

        Returns
        -------
        str
            File tag or extension for the LFP dataset
        &#34;&#34;&#34;

        return self._file_tag


    def get_timestamp(self):
        &#34;&#34;&#34;
        Returns the timestamps of the LFP waveform

        Parameters
        ----------
        None

        Returns
        -------
        ndarray
            Timestamps of the LFP signal

        &#34;&#34;&#34;

        return self._timestamp

    def _set_timestamp(self, timestamp=None):
        &#34;&#34;&#34;
        Sets the timestamps for LFP samples

        Parameters
        ----------
        timestamp : list or ndarray
            Timestamps of LFP samples

        Returns
        -------
        None

        &#34;&#34;&#34;

        if timestamp is not None:
            self._timestamp = timestamp

    def get_samples(self):
        &#34;&#34;&#34;
        Returns LFP waveform samples

        Parameters
        ----------
        None

        Returns
        -------
        ndarray
            Samples of the LFP signal

        &#34;&#34;&#34;

        return self._samples

    def _set_samples(self, samples=[]):
        &#34;&#34;&#34;
        Sets LFP samples

        Parameters
        ----------
        samples : list or ndarray
            LFP samples

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._samples = samples

    def _set_total_samples(self, tot_samples=0):
        &#34;&#34;&#34;
        Sets the number of LFP samples as part of storing the recording information

        Parameters
        ----------
        tot_samples : int
            Total number of samples in the LFP signal

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;No of samples&#39;] = tot_samples

    def _set_total_channel(self, tot_channels):
        &#34;&#34;&#34;
        Sets the value of number of channels as part of storing the recording information

        Parameters
        ----------
        tot_channels : int
            Total number of channels

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;No of channels&#39;] = tot_channels

    def _set_timestamp_bytes(self, bytes_per_timestamp):
        &#34;&#34;&#34;
        Sets `bytes per timestamp` value as part of storing the recording information

        Parameters
        ----------
        bytes_per_timestamp : int
            Total number of bytes to represent timestamp in the binary file

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;Bytes per timestamp&#39;] = bytes_per_timestamp

    def _set_sampling_rate(self, sampling_rate):
        &#34;&#34;&#34;
        Sets the sampling rate of the LFP signal as part of storing the recording information

        Parameters
        ----------
        sampling_rate : int
            Sampling rate of the LFP waveform

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;Sampling rate&#39;] = sampling_rate

    def _set_bytes_per_sample(self, bytes_per_sample):
        &#34;&#34;&#34;
        Sets `bytes per sample` value as part of storing the recording information

        Parameters
        ----------
        bytes_per_sample : int
            Total number of bytes to represent each sample in the binary file

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;Bytes per sample&#39;] = bytes_per_sample

    def _set_fullscale_mv(self, adc_fullscale_mv):
        &#34;&#34;&#34;
        Sets fullscale value of ADC value in mV as part of storing the recording information

        Parameters
        ----------
        adc_fullscale_mv : int
            Fullscale voltage of ADC signal in mV

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;ADC Fullscale mv&#39;] = adc_fullscale_mv

    def get_total_samples(self):
        &#34;&#34;&#34;
        Returns total number of LFP samples

        Parameters
        ----------
        None

        Returns
        -------
        ndarray
            Total number of LFP samples

        &#34;&#34;&#34;
        return self._record_info[&#39;No of samples&#39;]

    def get_total_channel(self):
        &#34;&#34;&#34;
        Returns total number of electrode channels in the LFP data file

        Parameters
        ----------
        None

        Returns
        -------
        int
            Total number of electrode channels
        &#34;&#34;&#34;

        return self._record_info[&#39;No of channels&#39;]

    def get_timestamp_bytes(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each timestamp in the binary file

        Parameters
        ----------
        None

        Returns
        -------
        int
            Number of bytes to represent timestamps

        &#34;&#34;&#34;

        return self._record_info[&#39;Bytes per timestamp&#39;]

    def get_sampling_rate(self):
        &#34;&#34;&#34;
        Returns the sampling rate of spike waveforms

        Parameters
        ----------
        None

        Returns
        -------
        int
            Sampling rate for spike waveforms

        &#34;&#34;&#34;

        return self._record_info[&#39;Sampling rate&#39;]

    def get_bytes_per_sample(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each LFP waveform sample

        Parameters
        ----------
        None

        Returns
        -------
        int
            Number of bytes to represent each sample of the LFP waveform

        &#34;&#34;&#34;

        return self._record_info[&#39;Bytes per sample&#39;]

    def get_fullscale_mv(self):
        &#34;&#34;&#34;
        Returns the fullscale value of the ADC in mV

        Parameters
        ----------
        None

        Returns
        -------
        int
            Fullscale ADC value in mV

        &#34;&#34;&#34;

        return self._record_info[&#39;ADC Fullscale mv&#39;]

    def get_recording_time(self):
        &#34;&#34;&#34;
        Returns the recording time in seconds

        Parameters
        ----------
        None

        Returns
        -------
        int
            Recording time in seconds
        &#34;&#34;&#34;

        return self.get_total_samples() / (self.get_sampling_rate())

    def load(self, filename=None, system=None):
        &#34;&#34;&#34;
        Loads LFP datasets

        Parameters
        ----------
        filename : str
            Name of the spike datafile
        system : str
            Recording system or format of the spike data file

        Returns
        -------
        None

        See also
        --------
        load_lfp_axona(), load_lfp_NLX(), load_lfp_NWB()

        &#34;&#34;&#34;
        if system is None:
            system = self._system
        else:
            self._system = system
        if filename is None:
            filename = self._filename
        else:
            self._filename = filename
        loader = getattr(self, &#39;load_lfp_&#39; + system)
        loader(filename)

    def add_spike(self, spike=None, **kwargs):
        &#34;&#34;&#34;
        Adds new spike node to current NLfp() object

        Parameters
        ----------
        spike : NSpikes
            NSPike object. If None, new object is created

        Returns
        -------
        `:obj:NSpike()`
            A new NSpike() object

        &#34;&#34;&#34;

        cls= kwargs.get(&#39;cls&#39;, None)
        if not inspect.isclass(cls):
            try:
                data_type = spike.get_type()
                if data_type == &#39;spike&#39;:
                    cls = spike.__class__
            except:
                 logging.error(&#39;Data type cannot be determined!&#39;)
        if inspect.isclass(cls):
             new_spike = self._add_node(cls, spike, &#39;spike&#39;, **kwargs)
             return new_spike
        else:
            logging.error(&#39;Cannot add the spike data!&#39;)


    def load_spike(self, names=&#39;all&#39;):
        &#34;&#34;&#34;
        Loads datasets of the spike nodes. Name of each node is used for obtaining the
        filenames

        Parameters
        ----------
        names : list of str
            Names of the nodes to load. If None, current NSpike() object is loaded

        Returns
        -------
        None

        &#34;&#34;&#34;


        if names == &#39;all&#39;:
            for spike in self._spikes:
                spike.load()
        else:
            logging.error(&#34;Spikes by name has yet to be implemented&#34;)
            # for name in names:
            #     spike = self.get_spikes_by_name(name)
            #     spike.load()

    def add_lfp(self, lfp=None, **kwargs):
        &#34;&#34;&#34;
        Adds new LFP node to current NLfp() object

        Parameters
        ----------
        lfp : NLfp
            NLfp object. If None, new object is created

        Returns
        -------
        `:obj:Nlfp`
            A new NLfp() object

        &#34;&#34;&#34;

        new_lfp = self._add_node(self.__class__, lfp, &#39;lfp&#39;, **kwargs)

        return new_lfp

    def load_lfp(self, names=None):
        &#34;&#34;&#34;
        Loads datasets of the LFP nodes. Name of each node is used for obtaining the
        filenames

        Parameters
        ----------
        names : list of str
            Names of the nodes to load. If `all`, all LFP nodes are loaded

        Returns
        -------
        None
        &#34;&#34;&#34;

        if names is None:
            self.load()
        elif names == &#39;all&#39;:
            for lfp in self._lfp:
                lfp.load()
        else:
            logging.error(&#34;Lfp by name has yet to be implemented&#34;)
            # for name in names:
            #     lfp = self.get_lfp_by_name(name)
            #     lfp.load()

    def spectrum(self, **kwargs):
        &#34;&#34;&#34;
        Analyses frequency spectrum of the LFP signal

        Parameters
        ----------
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        graph_data = oDict()

        Fs = self.get_sampling_rate()
        slc = kwargs.get(&#39;slice&#39;, None)
        if slc:
            lfp = self.get_samples()[slc]
        else:
            lfp = self.get_samples()

        window = kwargs.get(&#39;window&#39;, 1.0)
        window = sg.get_window(&#39;hann&#39;, int(window*Fs)) if isinstance(window, float)\
                or isinstance(window, int) else window

        win_sec = np.ceil(window.size/Fs)

        noverlap = kwargs.get(&#39;noverlap&#39;, 0.5*win_sec)
        noverlap = noverlap if noverlap &lt; win_sec else 0.5*win_sec
        noverlap = np.ceil(noverlap*Fs)

        nfft = kwargs.get(&#39;nfft&#39;, 2*Fs)
        nfft = np.power(2, int(np.ceil(np.log2(nfft))))

        ptype = kwargs.get(&#39;ptype&#39;, &#39;psd&#39;)
        ptype = &#39;spectrum&#39; if ptype == &#39;power&#39; else &#39;density&#39;

        prefilt = kwargs.get(&#39;prefilt&#39;, True)
        _filter = kwargs.get(&#39;filtset&#39;, [10, 1.5, 40, &#39;bandpass&#39;])

        fmax = kwargs.get(&#39;fmax&#39;, Fs/2)

        if prefilt:
            lfp = butter_filter(lfp, Fs, *_filter)

        tr = kwargs.get(&#39;tr&#39;, False)
        db = kwargs.get(&#39;db&#39;, False)
        if tr:
            f, t, Sxx = sg.spectrogram(lfp, fs=Fs, \
                    window=window, nperseg=window.size, noverlap=noverlap, nfft=nfft, \
                    detrend=&#39;constant&#39;, return_onesided=True, scaling=ptype)

            graph_data[&#39;t&#39;] = t
            graph_data[&#39;f&#39;] = f[find(f &lt;= fmax)]

            if db:
                Sxx = 10*np.log10(Sxx/np.amax(Sxx))
                Sxx = Sxx.flatten()
                Sxx[find(Sxx &lt; -40)] = -40
                Sxx = np.reshape(Sxx, [f.size, t.size])

#            graph_data[&#39;Sxx&#39;] = np.empty([find(f&lt;= fmax).size, t.size])
#            graph_data[&#39;Sxx&#39;] = np.array([Sxx[i, :] for i in find(f&lt;= fmax)])
            graph_data[&#39;Sxx&#39;] = Sxx[find(f &lt;= fmax), :]
        else:
            f, Pxx = sg.welch(lfp, fs=Fs, \
                    window=window, nperseg=window.size, noverlap=noverlap, nfft=nfft, \
                    detrend=&#39;constant&#39;, return_onesided=True, scaling=ptype)

            graph_data[&#39;f&#39;] = f[find(f &lt;= fmax)]

            if db:
                Pxx = 10*np.log10(Pxx/Pxx.max())
                Pxx[find(Pxx &lt; -40)] = -40
            graph_data[&#39;Pxx&#39;] = Pxx[find(f &lt;= fmax)]

        return graph_data

    def phase_dist(self, event_stamp, **kwargs):
        &#34;&#34;&#34;
        Analysis of spike to LFP phase distribution

        Parameters
        ----------
        evnet_stamp : ndarray
            Timestamps of the events of spiking activities for measring the phase
            distribution
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        _results= oDict()
        graph_data = oDict()

        cs = CircStat()

        lfp = self.get_samples()*1000
        Fs = self.get_sampling_rate()
        time = self.get_timestamp()

        # Input parameters
        bins = int(360/kwargs.get(&#39;binsize&#39;, 5))
        rbinsize = kwargs.get(&#39;rbinsize&#39;, 2) # raster binsize
        rbins = int(360/rbinsize)
        fwin = kwargs.get(&#39;fwin&#39;, [6, 12])
        pratio = kwargs.get(&#39;pratio&#39;, 0.2)
        aratio = kwargs.get(&#39;aratio&#39;, 0.15)

    # Filter
        fmax = fwin[1]
        fmin = fwin[0]
        _filter = [5, fmin, fmax, &#39;bandpass&#39;]
        _prefilt = kwargs.get(&#39;filtset&#39;, [10, 1.5, 40, &#39;bandpass&#39;])

        b_lfp = butter_filter(lfp, Fs, *_filter) # band LFP
        lfp = butter_filter(lfp, Fs, *_prefilt)

    # Measure phase
        hilb = sg.hilbert(b_lfp)
#        self.hilb = hilb
#        phase = np.remainder(np.angle(hilb, deg=True)+ 360, 360)
        phase = np.angle(hilb, deg=True)
        phase[phase &lt; 0] = phase[phase &lt; 0] + 360
        mag = np.abs(hilb)

        ephase = np.interp(event_stamp, time, phase)

        p2p = np.abs(np.max(lfp) - np.min(lfp))
        xline = 0.5* np.mean(mag) # cross line

        # Detection algo
        # zero cross
        mag1 = mag[0:-3]
        mag2 = mag[1:-2]
        mag3 = mag[2:-1]

        xind = np.union1d(find(np.logical_and(mag1 &lt; xline, mag2 &gt; xline)), \
                find(np.logical_and(np.logical_and(mag1 &lt; xline, mag2 == xline), mag3 &gt; xline)))

        # Ignore segments &lt;1/fmax
        i = 0
        rcount = np.empty([0,])
        bcount = np.empty([0, 0])

        phBins = np.arange(0, 360, 360/bins)
        rbins = np.arange(0, 360, 360/rbins)

        seg_count = 0
        while i &lt; len(xind)-1:
            k = i+1
            while time[xind[k]]- time[xind[i]] &lt; 1/fmin and k &lt; len(xind)-1:
                k += 1
#            print(time[xind[i]], time[xind[k]])
            s_lfp = lfp[xind[i]: xind[k]]
            s_p2p = np.abs(np.max(s_lfp)- np.min(s_lfp))

            if s_p2p &gt;= aratio*p2p:
                s_psd, f = fft_psd(s_lfp, Fs)
                if np.sum(s_psd[np.logical_and(f &gt;= fmin, f &lt;= fmax)]) &gt; pratio* np.sum(s_psd):
                    # Phase distribution
                    s_phase = ephase[np.logical_and(event_stamp &gt; time[xind[i]], event_stamp &lt;= time[xind[k]])]
#                    print(s_phase.shape, s_phase.shape)

                    if not s_phase.shape[0]:
                        pass
                    else:
                        seg_count += 1
                        cs.set_theta(s_phase)
                        temp_count = cs.circ_histogram(bins=rbinsize)
#                        temp_count = np.histogram(s_phase, bins=rbins, range=[0, 360])
                        temp_count = temp_count[0]
                        if not rcount.size:
                            rcount = temp_count
                        else:
                            rcount = np.append(rcount, temp_count)

                        temp_count = np.histogram(s_phase, bins=bins, range=[0, 360])
                        temp_count = np.resize(temp_count[0], [1, bins])
                        if not len(bcount):
                            bcount = temp_count
                        else:
                            bcount = np.append(bcount, temp_count, axis=0)
            i = k

        rcount = rcount.reshape([seg_count, rbins.size])

        phCount = np.sum(bcount, axis=0)

        cs.set_rho(phCount)
        cs.set_theta(phBins)

        cs.calc_stat()
        result = cs.get_result()
        meanTheta = result[&#39;meanTheta&#39;]*np.pi/180

        _results[&#39;LFP Spike Mean Phase&#39;]= result[&#39;meanTheta&#39;]
        _results[&#39;LFP Spike Mean Phase Count&#39;]= result[&#39;meanRho&#39;]
        _results[&#39;LFP Spike Phase Res Vect&#39;]= result[&#39;resultant&#39;]

        graph_data[&#39;meanTheta&#39;] = meanTheta
        graph_data[&#39;phCount&#39;] = phCount
        graph_data[&#39;phBins&#39;] = phBins
        graph_data[&#39;raster&#39;] = rcount
        graph_data[&#39;rasterbins&#39;] = rbins

        self.update_result(_results)

        return graph_data

    def phase_at_events(self, event_stamps, **kwargs):
        &#34;&#34;&#34;
        Phase based on times.

        Parameters
        ----------
        event_stamps : array
            an array of event times
        **kwargs:
            keyword arguments

        Returns
        -------
            (array)
            Phase values for each position
        &#34;&#34;&#34;
        lfp = self.get_samples() * 1000
        Fs = self.get_sampling_rate()
        time = self.get_timestamp()

        # Input parameters
        fwin = kwargs.get(&#39;fwin&#39;, [6, 12])

        # Filter
        fmax = fwin[1]
        fmin = fwin[0]
        _filter = [5, fmin, fmax, &#39;bandpass&#39;]
        _prefilt = kwargs.get(&#39;filtset&#39;, [10, 1.5, 40, &#39;bandpass&#39;])

        b_lfp = butter_filter(lfp, Fs, *_filter)  # band LFP
        lfp = butter_filter(lfp, Fs, *_prefilt)

        # Measure phase
        hilb = sg.hilbert(b_lfp)
        phase = np.angle(hilb, deg=True)
        phase[phase &lt; 0] = phase[phase &lt; 0] + 360

        ephase = np.interp(event_stamps, time, phase)

        return ephase

    def plv(self, event_stamp, **kwargs):
        &#34;&#34;&#34;
        Calculates phase-locking value of the spike train to underlying LFP signal.

        When &#39;mode&#39;= None in the inpput kwargs, it calculates the PLV and SFC over
        the entire spike-train.

        If &#39;mode&#39;= &#39;bs&#39;, it bootstraps the spike-timestamps
        and calculates the locking values for each set of new spike timestamps.

        If &#39;mode&#39;= &#39;tr&#39;, a time-resilved phase-locking analysis is performed where
        the LFP signal is split into overlapped segments for each calculation.

        Parameters
        ----------
        evnet_stamp : ndarray
            Timestamps of the events or the spiking activities for measuring the phase
            locking
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        graph_data = oDict()

        lfp = self.get_samples()*1000
        Fs = self.get_sampling_rate()
        time = self.get_timestamp()

        window = np.array(kwargs.get(&#39;window&#39;, [-0.5, 0.5]))
        win = np.ceil(window*Fs).astype(int)
        win = np.arange(win[0], win[1])
        slep_win = sg.hann(win.size, False)

        nfft = kwargs.get(&#39;nfft&#39;, 1024)
        mode = kwargs.get(&#39;mode&#39;, None) # None, &#39;bs&#39;, &#39;tr&#39; bs=bootstrp, tr=time-resolved
        fwin = kwargs.get(&#39;fwin&#39;, [])

        xf = np.arange(0, Fs, Fs/nfft)
        f = xf[0: int(nfft/2)+ 1]

        ind = np.arange(f.size) if len(fwin) == 0 else find(np.logical_and(f &gt;= fwin[0], f &lt;= fwin[1]))

        if mode == &#39;bs&#39;:
            nsample = kwargs.get(&#39;nsample&#39;, 50)
            nrep = kwargs.get(&#39;nrep&#39;, 500)

            STA = np.empty([nrep, win.size])
            fSTA = np.empty([nrep, ind.size])
            STP = np.empty([nrep, ind.size])
            SFC = np.empty([nrep, ind.size])
            PLV = np.empty([nrep, ind.size])

            for i in np.arange(nrep):
                data = self.plv(np.random.choice(event_stamp, nsample, False), \
                        window=window, nfft=nfft, mode=None, fwin=fwin)
                t = data[&#39;t&#39;]
                STA[i, :] = data[&#39;STA&#39;]
                fSTA[i, :] = data[&#39;fSTA&#39;]
                STP[i, :] = data[&#39;STP&#39;]
                SFC[i, :] = data[&#39;SFC&#39;]
                PLV[i, :] = data[&#39;PLV&#39;]

            graph_data[&#39;t&#39;] = t
            graph_data[&#39;f&#39;] = f[ind]
            graph_data[&#39;STAm&#39;] = STA.mean(0)
            graph_data[&#39;fSTAm&#39;] = fSTA.mean(0)
            graph_data[&#39;STPm&#39;] = STP.mean(0)
            graph_data[&#39;SFCm&#39;] = SFC.mean(0)
            graph_data[&#39;PLVm&#39;] = PLV.mean(0)

            graph_data[&#39;STAe&#39;] = stats.sem(STA, 0)
            graph_data[&#39;fSTAe&#39;] = stats.sem(fSTA, 0)
            graph_data[&#39;STPe&#39;] = stats.sem(STP, 0)
            graph_data[&#39;SFCe&#39;] = stats.sem(SFC, 0)
            graph_data[&#39;PLVe&#39;] = stats.sem(PLV, 0)

        elif mode == &#39;tr&#39;:
            nsample = kwargs.get(&#39;nsample&#39;, None)

            slide = kwargs.get(&#39;slide&#39;, 25) # in ms
            slide = slide/1000 # convert to sec

            offset = np.arange(window[0], window[-1], slide)
            nwin = offset.size

            fSTA = np.empty([nwin, ind.size])
            STP = np.empty([nwin, ind.size])
            SFC = np.empty([nwin, ind.size])
            PLV = np.empty([nwin, ind.size])

            if nsample is None or nsample &gt; event_stamp.size:
                stamp = event_stamp
            else:
                stamp = np.random.choice(event_stamp, nsample, False)

            for i in np.arange(nwin):
                data = self.plv(stamp + offset[i], \
                        nfft=nfft, mode=None, fwin=fwin, window=window)
                t = data[&#39;t&#39;]
                fSTA[i, :] = data[&#39;fSTA&#39;]
                STP[i, :] = data[&#39;STP&#39;]
                SFC[i, :] = data[&#39;SFC&#39;]
                PLV[i, :] = data[&#39;PLV&#39;]

            graph_data[&#39;offset&#39;] = offset
            graph_data[&#39;f&#39;] = f[ind]
            graph_data[&#39;fSTA&#39;] = fSTA.transpose()
            graph_data[&#39;STP&#39;] = STP.transpose()
            graph_data[&#39;SFC&#39;] = SFC.transpose()
            graph_data[&#39;PLV&#39;] = PLV.transpose()

        elif mode is None:
            center = time.searchsorted(event_stamp)
            # Keep windows within data
            center = np.array([center[i] for i in range(0, len(event_stamp)) \
                if center[i] + win[0] &gt;= 0 and center[i] + win[-1] &lt;= time.size])

            sta_data = self.event_trig_average(event_stamp, **kwargs)
            STA = sta_data[&#39;ETA&#39;]

            fSTA = fft(np.multiply(STA, slep_win), nfft)

            fSTA = np.absolute(fSTA[0: int(nfft/2)+ 1])**2/nfft**2
            fSTA[1:-1] = 2*fSTA[1:-1]

            fLFP = np.array([fft(np.multiply(lfp[x+ win], slep_win), nfft) \
                    for x in center])

            STP = np.absolute(fLFP[:, 0: int(nfft/2)+ 1])**2/nfft**2
            STP[:, 1:-1] = 2*STP[:, 1:-1]
            STP = STP.mean(0)

            SFC = np.divide(fSTA, STP)*100

            PLV = np.copy(fLFP)

            # Normalize
            PLV = np.divide(PLV, np.absolute(PLV))
            PLV[np.isnan(PLV)] = 0

            PLV = np.absolute(PLV.mean(0))[0: int(nfft/2)+ 1]
            PLV[1:-1] = 2*PLV[1:-1]

            graph_data[&#39;t&#39;] = sta_data[&#39;t&#39;]
            graph_data[&#39;f&#39;] = f[ind]
            graph_data[&#39;STA&#39;] = STA
            graph_data[&#39;fSTA&#39;] = fSTA[ind]
            graph_data[&#39;STP&#39;] = STP[ind]
            graph_data[&#39;SFC&#39;] = SFC[ind]
            graph_data[&#39;PLV&#39;] = PLV[ind]

        return graph_data

    def event_trig_average(self, event_stamp=None, **kwargs):
        &#34;&#34;&#34;
        Averaging event-triggered LFP signals

        Parameters
        ----------
        event_stamp : ndarray
            Timestamps of the events or the spiking activities for measuring the
            event triggered average of the LFP signal
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        graph_data = oDict()
        window = np.array(kwargs.get(&#39;window&#39;, [-0.5, 0.5]))

        if event_stamp is None:
            spike = kwargs.get(&#39;spike&#39;, None)

            try:
                data_type = spike.get_type()
            except:
                logging.error(&#39;The data type of the addes object cannot be determined!&#39;)

            if data_type == &#39;spike&#39;:
                event_stamp = spike.get_unit_stamp()
            elif spike in self.get_spike_names():
                event_stamp = self.get_spike(spike).get_unit_stamp()

        if event_stamp is None:
            logging.error(&#39;No valid event timestamp or spike is provided&#39;)
        else:
            lfp = self.get_samples()*1000
            Fs = self.get_sampling_rate()
            time = self.get_timestamp()
            center = time.searchsorted(event_stamp, side=&#39;left&#39;)
            win = np.ceil(window*Fs).astype(int)
            win = np.arange(win[0], win[1])

            # Keep windows within data
            center = np.array([center[i] for i in range(0, len(event_stamp)) \
                if center[i]+ win[0] &gt;= 0 and center[i]+ win[-1] &lt;= time.size])

            eta = reduce(lambda y, x: y+ lfp[x+ win], center)
            eta = eta/center.size

            graph_data[&#39;t&#39;] = win/Fs
            graph_data[&#39;ETA&#39;] = eta
            graph_data[&#39;center&#39;] = center

        return graph_data

    def spike_lfp_causality(self, spike=None, **kwargs):
        &#34;&#34;&#34;
        (Not implemented yet)

        Analyses spike to underlying LFP causality

        Parameters
        ----------
        spike : NSpike
            Spike dataset which is used for the causality analysis
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Should return graphical data of the analysis. The function is not
            implemented yet.

        &#34;&#34;&#34;

        pass

    def subsample(self, sample_range=None):
        &#34;&#34;&#34;
        Extract a time range from the lfp.

        Parameters
        ----------
        sample_range : tuple
            the time in seconds to extract from the lfp

        Returns
        -------
        NLfp
            subsampled version of initial lfp object
        &#34;&#34;&#34;
        in_range = sample_range
        sample_rate = self.get_sampling_rate()
        if in_range is None:
            length = int(self.get_duration() * sample_rate)
            if (length != self.get_total_samples()):
                logging.warning(
                    &#34;Unequal calculated and recorded total lfp samples&#34; +
                    &#34;Calculated {} and recorded {}&#34;.format(
                        length, self.get_total_samples()))
            return self
        else:
            new_lfp = deepcopy(self)
            lfp_samples = self.get_samples()[
                int(sample_rate * in_range[0]):int(sample_rate * in_range[1])]
            lfp_times = self.get_timestamp()[
                int(sample_rate * in_range[0]):int(sample_rate * in_range[1])]
            new_lfp._set_samples(lfp_samples)
            new_lfp._set_timestamp(lfp_times)
            new_lfp._set_total_samples(len(lfp_samples))
            new_lfp._set_duration(in_range[1] - in_range[0])
            return new_lfp

    def sharp_wave_ripples(self, in_range=None, **kwargs):
        &#34;&#34;&#34;
        Detect SWR events in the lfp, optionally in a given range

        Parameters
        ----------
        in_range : tuple
            A range in seconds

        kwargs
        ------
        swr_lower : float
            Lower band in hz
        swr_upper : float
            Upper band in hz
        rms_window_size_ms : int
            Size of the rms window in ms
        percentile : float
            The percentile threshold for a peak

        Returns
        -------
        dict
            lfp times, lfp samples, swr times, lfp sample rate

        &#34;&#34;&#34;
        swr_lower = kwargs.get(&#34;swr_lower&#34;, 100)
        swr_higher = kwargs.get(&#34;swr_upper&#34;, 250)
        rms_window_size_ms = kwargs.get(&#34;rms_window_size_ms&#34;, 7)
        percentile = kwargs.get(&#34;peak_percentile&#34;, 99.5)

        lfp = self.subsample(in_range)
        sample_rate = lfp.get_sampling_rate()
        # Estimate SWR events
        filtered_lfp = butter_filter(
            lfp.get_samples(), sample_rate, 10,
            swr_lower, swr_higher, &#39;bandpass&#39;)
        rms_window_size = floor((rms_window_size_ms / 1000) * sample_rate)
        rms_envelope = window_rms(filtered_lfp, rms_window_size, mode=&#34;same&#34;)
        p_val = np.percentile(rms_envelope, percentile)
        _, peaks = find_peaks(rms_envelope, thresh=p_val)
        peaks = lfp.get_timestamp()[0] + (peaks / sample_rate)

        &#34;&#34;&#34;
        Alternative way to get SWR
        #rms_envelope = distinct_window_rms(filtered_lfp, rms_window_size)
        #peaks = (
        # longest_sleep_period[0] + peaks * rms_window_size) / sample_rate
        &#34;&#34;&#34;

        return {
            &#34;lfp times&#34;: lfp.get_timestamp(),
            &#34;lfp samples&#34;: filtered_lfp,
            &#34;swr times&#34;: peaks, &#34;lfp sample rate&#34;: sample_rate}

    def bandpower(self, band, **kwargs):
        &#34;&#34;&#34;Compute the average power of the signal x in a specific frequency band.

        Modified from excellent article at
        https://raphaelvallat.com/bandpower.html

        Parameters
        ----------
        band : list
        Lower and upper frequencies of the band of interest.

        kwargs:
            method : string
                Periodogram method: &#39;welch&#39;
            window_sec : float
                Length of each window in seconds.
                If None, window_sec = (1 / min(band)) * 2.
            band_total : bool
                Whether to band the total power
                Default False
            total_band: List
                low and high frequency values for the filter
                Default [1.5, 40]

        Returns
        ------
        bp : Dict
            &#34;bandpower&#34;, &#34;total_power&#34; and &#34;relative_power&#34;.
        &#34;&#34;&#34;
        from scipy.signal import welch
        from scipy.integrate import simps

        band = np.asarray(band)
        low, high = band
        method = kwargs.get(&#34;method&#34;, &#34;welch&#34;)
        window_sec = kwargs.get(&#34;window_sec&#34;, 2 / (low + 0.000001))
        sf = self.get_sampling_rate()
        lfp_samples = self.get_samples()

        band_total = kwargs.get(&#39;band_total&#39;, False)
        _filter = kwargs.get(&#39;total_band&#39;, [1.5, 40])

        # if prefilt:
        #     lfp_samples = butter_filter(lfp_samples, sf, *_filter)
        # Compute the modified periodogram (Welch)
        if method == &#39;welch&#39;:
            nperseg = int(window_sec * sf)
            freqs, psd = welch(lfp_samples, sf, nperseg=nperseg)

        # The multaper method is more accurate but we will not use it
        # Welch&#39;s method is still very good
        # See MNE for the multitaper method
        # from mne.time_frequency import psd_array_multitaper
        # elif method == &#39;multitaper&#39;:
        #     psd, freqs = psd_array_multitaper(lfp_samples, sf, adaptive=True,
        #                                     normalization=&#39;full&#39;, verbose=0)

        # Frequency resolution
        freq_res = freqs[1] - freqs[0]

        # Find index of band in frequency vector
        idx_band = np.logical_and(freqs &gt;= low, freqs &lt;= high)

        # Integral approximation of the spectrum using parabola (Simpson&#39;s rule)
        bp = simps(psd[idx_band], dx=freq_res)

        if band_total:
            idx_band = np.logical_and(
                freqs &gt;= _filter[0],
                freqs &lt;= _filter[1])
            tp = simps(psd[idx_band], dx=freq_res)
        else:
            tp = simps(psd, dx=freq_res)

        output = {
            &#34;bandpower&#34;: bp,
            &#34;total_power&#34;: tp,
            &#34;relative_power&#34;: bp/tp}
        return output

    def bandpower_ratio(self, first_band, second_band, win_sec, **kwargs):
        &#34;&#34;&#34;
        Calculate the ratio in power between two bandpass filtered signals.

        Note that common ranges are:
        delta (1.5–4 Hz), theta (5-11 Hz)

        Parameters
        ----------
        first_band - 1d array
            lower and upper bands
        second_band - 1d array
            lower and upper bands
        win_sec - float
            length of the windows to bin lfp into in seconds.
            recommend 4 for eg.
        kwargs:
            first_name - str name of band 1, default &#34;Band 1&#34;
            second_name - str name of band 2, default &#34;Band 2&#34;

        Returns
        -------
        float - the ratio between the power signals.

        See also
        --------
        nc_lfp.NLfp().bandpower()
        &#34;&#34;&#34;

        _results = oDict()
        name1 = kwargs.get(&#34;first_name&#34;, &#34;Band 1&#34;)
        name2 = kwargs.get(&#34;second_name&#34;, &#34;Band 2&#34;)
        if &#34;window_sec&#34; not in kwargs:
            kwargs[&#34;window_sec&#34;] = win_sec

        b1 = self.bandpower(first_band, **kwargs)
        b2 = self.bandpower(second_band, **kwargs)
        if b1[&#34;total_power&#34;] != b2[&#34;total_power&#34;]:
            logging.error(
                &#34;Differing total power in lfp bandpower ratio calculations&#34;)
        bp = b1[&#34;bandpower&#34;] / b2[&#34;bandpower&#34;]
        key1 = name1 + &#34; Power&#34;
        key2 = name2 + &#34; Power&#34;
        key3 = name1 + &#34; &#34; + name2 + &#34; Power Ratio&#34;
        _results[key1] = b1[&#34;bandpower&#34;]
        _results[key1 + &#34; (Relative)&#34;] = b1[&#34;relative_power&#34;]
        _results[key2] = b2[&#34;bandpower&#34;]
        _results[key2 + &#34; (Relative)&#34;] = b2[&#34;relative_power&#34;]
        _results[key3] = bp
        _results[&#34;Total Power&#34;] = b1[&#34;total_power&#34;]
        self.update_result(_results)
        return bp

    def save_to_hdf5(self, file_name=None, system=None):
        &#34;&#34;&#34;
        Stores NLfp() object to HDF5 file

        Parameters
        ----------
        file_name : str
            Full file directory for the lfp data
        system : str
            Recoring system or data format

        Returns
        -------
        None

        Also see
        --------
        nc_hdf.Nhdf().save_lfp()

        &#34;&#34;&#34;

        hdf = Nhdf()
        if file_name and system:
            if os.path.exists(file_name):
                self.set_filename(file_name)
                self.set_system(system)
                self.load()
            else:
                logging.error(&#39;Specified file cannot be found!&#39;)

        hdf.save_lfp(lfp=self)
        hdf.close()

    def load_lfp_NWB(self, file_name):
        &#34;&#34;&#34;
        Decodes LFP data from NWB (HDF5) file format

        Parameters
        ----------
        file_name : str
            Full file directory for the lfp data

        Returns
        -------
        None

        &#34;&#34;&#34;

        file_name, path = file_name.split(&#39;+&#39;)
        if os.path.exists(file_name):
            hdf = Nhdf()
            hdf.set_filename(file_name)

            _record_info = {}

            if path in hdf.f:
                g = hdf.f[path]
            elif &#39;/processing/Neural Continuous/LFP/&#39;+ path in hdf.f:
                path = &#39;/processing/Neural Continuous/LFP/&#39;+ path
                g = hdf.f[path]
            else:
                logging.error(&#39;Specified path does not exist!&#39;)

            for key, value in g.attrs.items():
                _record_info[key] = value

            self.set_record_info(_record_info)

            self._set_samples(hdf.get_dataset(group=g, name=&#39;data&#39;))
            self._set_timestamp(hdf.get_dataset(group=g, name=&#39;timestamps&#39;))
            self._set_total_samples(hdf.get_dataset(group=g, name=&#39;num_samples&#39;))

            hdf.close()
        else:
            logging.error(file_name + &#39; does not exist!&#39;)

    def load_lfp_Axona(self, file_name):
        &#34;&#34;&#34;
        Decodes LFP data from Axona file format

        Parameters
        ----------
        file_name : str
            Full file directory for the lfp data

        Returns
        -------
        None

        &#34;&#34;&#34;

        words = file_name.split(sep=os.sep)
        file_directory = os.sep.join(words[0:-1])
        file_tag, file_extension = words[-1].split(&#39;.&#39;)
        set_file = file_directory + os.sep + file_tag + &#39;.set&#39;

        self._set_data_source(file_name)
        self._set_source_format(&#39;Axona&#39;)

        if os.path.isfile(file_name):
            with open(file_name, &#39;rb&#39;) as f:
                while True:
                    line = f.readline()
                    try:
                        line = line.decode(&#39;latin-1&#39;)
                    except:
                        break

                    if line == &#39;&#39;:
                        break
                    if line.startswith(&#39;trial_date&#39;):
                        self._set_date(&#39; &#39;.join(line.replace(&#39;,&#39;, &#39; &#39;).split()[1:]))
                    if line.startswith(&#39;trial_time&#39;):
                        self._set_time(line.split()[1])
                    if line.startswith(&#39;experimenter&#39;):
                        self._set_experiemnter(&#39; &#39;.join(line.split()[1:]))
                    if line.startswith(&#39;comments&#39;):
                        self._set_comments(&#39; &#39;.join(line.split()[1:]))
                    if line.startswith(&#39;duration&#39;):
                        self._set_duration(float(&#39;&#39;.join(line.split()[1:])))
                    if line.startswith(&#39;sw_version&#39;):
                        self._set_file_version(line.split()[1])
                    if line.startswith(&#39;num_chans&#39;):
                        self._set_total_channel(int(&#39;&#39;.join(line.split()[1:])))
                    if line.startswith(&#39;sample_rate&#39;):
                        self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                    if line.startswith(&#39;bytes_per_sample&#39;):
                        self._set_bytes_per_sample(int(&#39;&#39;.join(line.split()[1:])))
                    if line.startswith(&#39;num_&#39;+ file_extension[:3].upper() + &#39;_samples&#39;):
                        self._set_total_samples(int(&#39;&#39;.join(line.split()[1:])))
                    if line.startswith(&#34;data_start&#34;):
                        break

                num_samples = self.get_total_samples()
                bytes_per_sample = self.get_bytes_per_sample()

                f.seek(0, 0)
                header_offset = []
                while True:
                    try:
                        buff = f.read(10).decode(&#39;UTF-8&#39;)
                    except:
                        break
                    if buff == &#39;data_start&#39;:
                        header_offset = f.tell()
                        break
                    else:
                        f.seek(-9, 1)

                eeg_ID = re.findall(r&#39;\d+&#39;, file_extension)
                self.set_file_tag(1 if not eeg_ID else int(eeg_ID[0]))
                max_ADC_count = 2**(8*bytes_per_sample-1)-1
                max_byte_value = 2**(8*bytes_per_sample)

                with open(set_file, &#39;r&#39;, encoding=&#39;latin-1&#39;) as f_set:
                    lines = f_set.readlines()
                    channel_lines = dict([tuple(map(int, re.findall(r&#39;\d+.\d+|\d+&#39;, line)[0].split()))\
                                for line in lines if line.startswith(&#39;EEG_ch_&#39;)])
                    channel_id = channel_lines[self.get_file_tag()]
                    self.set_channel_id(channel_id)

                    gain_lines = dict([tuple(map(int, re.findall(r&#39;\d+.\d+|\d+&#39;, line)[0].split()))\
                            for line in lines if &#39;gain_ch_&#39; in line])
                    gain = gain_lines[channel_id-1]

                    for line in lines:
                        if line.startswith(&#39;ADC_fullscale_mv&#39;):
                            self._set_fullscale_mv(int(re.findall(r&#39;\d+.\d+|d+&#39;, line)[0]))
                            break
                    AD_bit_uvolt = 2*self.get_fullscale_mv()/ \
                                    (gain*np.power(2, 8*bytes_per_sample))

                record_size = bytes_per_sample
                sample_le = 256**(np.arange(0, bytes_per_sample, 1))

                if not header_offset:
                    print(&#39;Error: data_start marker not found!&#39;)
                else:
                    f.seek(header_offset, 0)
                    byte_buffer = np.fromfile(f, dtype=&#39;uint8&#39;)
                    len_bytebuffer = len(byte_buffer)
                    end_offset = len(&#39;\r\ndata_end\r&#39;)
                    lfp_wave = np.zeros([num_samples, ], dtype=np.float64)
                    for k in np.arange(0, bytes_per_sample, 1):
                        byte_offset = k
                        sample_value = (sample_le[k]* byte_buffer[byte_offset \
                                    :byte_offset+ len_bytebuffer- end_offset- record_size\
                                    :record_size])
                        if sample_value.size &lt; num_samples:
                            sample_value = np.append(sample_value, np.zeros([num_samples-sample_value.size,]))
                        sample_value = sample_value.astype(np.float64, casting=&#39;unsafe&#39;, copy=False)
                        np.add(lfp_wave, sample_value, out=lfp_wave)
                    np.putmask(lfp_wave, lfp_wave &gt; max_ADC_count, lfp_wave- max_byte_value)

                    self._set_samples(lfp_wave*AD_bit_uvolt)
                    self._set_timestamp(np.arange(0, num_samples, 1)/self.get_sampling_rate())

        else:
            logging.error(
                &#34;No lfp file found for file {}&#34;.format(file_name))

    def load_lfp_Neuralynx(self, file_name):
        &#34;&#34;&#34;
        Decodes LFP data from Neuralynx file format

        Parameters
        ----------
        file_name : str
            Full file directory for the lfp data

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._set_data_source(file_name)
        self._set_source_format(&#39;Neuralynx&#39;)

        # Format description for the NLX file:

        resamp_freq = 250 # NeuroChaT subsamples the original recording from 32000 to 250

        header_offset = 16*1024 # fixed for NLX files

        bytes_per_timestamp = 8
        bytes_chan_no = 4
        bytes_sample_freq = 4
        bytes_num_valid_samples = 4
        bytes_per_sample = 2
        samples_per_record = 512

        max_byte_value = np.power(2, bytes_per_sample*8)
        max_ADC_count = np.power(2, bytes_per_sample*8- 1)-1
        AD_bit_uvolt = 10**-6

        self._set_bytes_per_sample(bytes_per_sample)

        record_size = None
        with open(file_name, &#39;rb&#39;) as f:
            while True:
                line = f.readline()
                try:
                    line = line.decode(&#39;UTF-8&#39;)
                except:
                    break

                if line == &#39;&#39;:
                    break
                if &#39;SamplingFrequency&#39; in line:
                    self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))) # We are subsampling from the blocks of 512 samples per record
                if &#39;RecordSize&#39; in line:
                    record_size = int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;Time Opened&#39; in line:
                    self._set_date(re.search(r&#39;\d+/\d+/\d+&#39;, line).group())
                    self._set_time(re.search(r&#39;\d+:\d+:\d+&#39;, line).group())
                if &#39;FileVersion&#39; in line:
                    self._set_file_version(line.split()[1])
                if &#39;ADMaxValue&#39; in line:
                    max_ADC_count = float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;ADBitVolts&#39; in line:
                    AD_bit_uvolt = float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))*(10**6)

            self._set_fullscale_mv(max_byte_value*AD_bit_uvolt/2) # gain = 1 assumed to keep in similarity to Axona

            if not record_size:
                record_size = bytes_per_timestamp+ \
                             bytes_chan_no+ \
                             bytes_sample_freq+ \
                             bytes_num_valid_samples+ \
                             bytes_per_sample*samples_per_record

            time_offset = 0
            sample_freq_offset = bytes_per_timestamp+ bytes_chan_no
            num_valid_samples_offset = sample_freq_offset+ bytes_sample_freq
            sample_offset = num_valid_samples_offset+ bytes_num_valid_samples
            f.seek(0, 2)
            num_samples = int((f.tell()- header_offset)/record_size)

            f.seek(header_offset, 0)
            time = np.array([])
            lfp_wave = np.array([])
            sample_le = 256**(np.arange(0, bytes_per_sample, 1))
            for _ in np.arange(num_samples):
                sample_bytes = np.fromfile(f, dtype=&#39;uint8&#39;, count=record_size)
                block_start = int.from_bytes(sample_bytes[time_offset+ \
                                np.arange(bytes_per_timestamp)], byteorder=&#39;little&#39;, signed=False)/10**6
                valid_samples = int.from_bytes(sample_bytes[num_valid_samples_offset+ \
                                np.arange(bytes_num_valid_samples)], byteorder=&#39;little&#39;, signed=False)
                sampling_freq = int.from_bytes(sample_bytes[sample_freq_offset+ \
                                np.arange(bytes_sample_freq)], byteorder=&#39;little&#39;, signed=False)

                wave_bytes = sample_bytes[sample_offset+ np.arange(valid_samples* bytes_per_sample)]\
                                .reshape([valid_samples, bytes_per_sample])
                block_wave = np.dot(wave_bytes, sample_le)
                #    for k in np.arange(valid_samples):
                #        block_wave[k] = int.from_bytes(sample_bytes[sample_offset+ k*bytes_per_sample+ \
                #                    np.arange(bytes_per_sample)], byteorder=&#39;little&#39;, signed=False)
                np.putmask(block_wave, block_wave &gt; max_ADC_count, block_wave - max_byte_value)
                block_wave = block_wave*AD_bit_uvolt
                block_time = block_start +  np.arange(valid_samples)/ sampling_freq
                interp_time = np.arange(block_start, block_time[-1], 1/resamp_freq)
                interp_wave = np.interp(interp_time, block_time, block_wave)
                time = np.append(time, interp_time)
                lfp_wave = np.append(lfp_wave, interp_wave)
            time -= time.min()
            self._set_samples(lfp_wave)
            self._set_total_samples(lfp_wave.size)
            self._set_timestamp(time)
            self._set_sampling_rate(resamp_freq)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="neurochat.nc_lfp.NLfp"><code class="flex name class">
<span>class <span class="ident">NLfp</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>This data class is the placeholder for the dataset that contains information
about the neural LFP signal. It decodes data from different formats and analyses
LFP signal in the recording.</p>
<p>Instantiate the <code>NBase</code> class</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keyword arguments</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">class NLfp(NBase):
    &#34;&#34;&#34;
    This data class is the placeholder for the dataset that contains information
    about the neural LFP signal. It decodes data from different formats and analyses
    LFP signal in the recording.

    &#34;&#34;&#34;

    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self._file_tag = &#39;&#39;
        self._channel_id = 0
        self._samples = None
        self._timestamp = None
        self.set_record_info({&#39;Total samples&#39;: 0})

        self.__type = &#39;lfp&#39;

    def get_type(self):
        &#34;&#34;&#34;
        Returns the type of object. For NLfp, this is always `lfp` type

        Parameters
        ----------
        None

        Returns
        -------
        str

        &#34;&#34;&#34;
        return self.__type

    # For multi-unit analysis, {&#39;SpikeName&#39;: cell_no} pairs should be used as function input

    def set_channel_id(self, channel_id=&#39;&#39;):
        &#34;&#34;&#34;
        Sets the electrode channels ID

        Parameters
        ----------
        channel_id : str
            Channel ID for the LFP data

        Returns
        -------
        None

        &#34;&#34;&#34;
        self._channel_id = channel_id

    def get_channel_id(self):
        &#34;&#34;&#34;
        Returns the electrode channels ID

        Parameters
        ----------
        None

        Returns
        -------
        str
            LFP channel ID

        &#34;&#34;&#34;

        return self._channel_id

    def set_file_tag(self, file_tag):
        &#34;&#34;&#34;
        Sets the file tag or extension for the LFP dataset. For example, Axona recordings usually
        have file tags like &#39;eeg&#39; or &#39;eeg8&#39; etc.

        Parameters
        ----------
        file_tag : str
            File tag or extension for the LFP dataset

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._file_tag = file_tag

    def get_file_tag(self):
        &#34;&#34;&#34;
        Returns the file tag or extension for the LFP dataset. For example, Axona recordings usually
        have file tags like &#39;eeg&#39; or &#39;eeg8&#39; etc.

        Parameters
        ----------
        None

        Returns
        -------
        str
            File tag or extension for the LFP dataset
        &#34;&#34;&#34;

        return self._file_tag


    def get_timestamp(self):
        &#34;&#34;&#34;
        Returns the timestamps of the LFP waveform

        Parameters
        ----------
        None

        Returns
        -------
        ndarray
            Timestamps of the LFP signal

        &#34;&#34;&#34;

        return self._timestamp

    def _set_timestamp(self, timestamp=None):
        &#34;&#34;&#34;
        Sets the timestamps for LFP samples

        Parameters
        ----------
        timestamp : list or ndarray
            Timestamps of LFP samples

        Returns
        -------
        None

        &#34;&#34;&#34;

        if timestamp is not None:
            self._timestamp = timestamp

    def get_samples(self):
        &#34;&#34;&#34;
        Returns LFP waveform samples

        Parameters
        ----------
        None

        Returns
        -------
        ndarray
            Samples of the LFP signal

        &#34;&#34;&#34;

        return self._samples

    def _set_samples(self, samples=[]):
        &#34;&#34;&#34;
        Sets LFP samples

        Parameters
        ----------
        samples : list or ndarray
            LFP samples

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._samples = samples

    def _set_total_samples(self, tot_samples=0):
        &#34;&#34;&#34;
        Sets the number of LFP samples as part of storing the recording information

        Parameters
        ----------
        tot_samples : int
            Total number of samples in the LFP signal

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;No of samples&#39;] = tot_samples

    def _set_total_channel(self, tot_channels):
        &#34;&#34;&#34;
        Sets the value of number of channels as part of storing the recording information

        Parameters
        ----------
        tot_channels : int
            Total number of channels

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;No of channels&#39;] = tot_channels

    def _set_timestamp_bytes(self, bytes_per_timestamp):
        &#34;&#34;&#34;
        Sets `bytes per timestamp` value as part of storing the recording information

        Parameters
        ----------
        bytes_per_timestamp : int
            Total number of bytes to represent timestamp in the binary file

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;Bytes per timestamp&#39;] = bytes_per_timestamp

    def _set_sampling_rate(self, sampling_rate):
        &#34;&#34;&#34;
        Sets the sampling rate of the LFP signal as part of storing the recording information

        Parameters
        ----------
        sampling_rate : int
            Sampling rate of the LFP waveform

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;Sampling rate&#39;] = sampling_rate

    def _set_bytes_per_sample(self, bytes_per_sample):
        &#34;&#34;&#34;
        Sets `bytes per sample` value as part of storing the recording information

        Parameters
        ----------
        bytes_per_sample : int
            Total number of bytes to represent each sample in the binary file

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;Bytes per sample&#39;] = bytes_per_sample

    def _set_fullscale_mv(self, adc_fullscale_mv):
        &#34;&#34;&#34;
        Sets fullscale value of ADC value in mV as part of storing the recording information

        Parameters
        ----------
        adc_fullscale_mv : int
            Fullscale voltage of ADC signal in mV

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._record_info[&#39;ADC Fullscale mv&#39;] = adc_fullscale_mv

    def get_total_samples(self):
        &#34;&#34;&#34;
        Returns total number of LFP samples

        Parameters
        ----------
        None

        Returns
        -------
        ndarray
            Total number of LFP samples

        &#34;&#34;&#34;
        return self._record_info[&#39;No of samples&#39;]

    def get_total_channel(self):
        &#34;&#34;&#34;
        Returns total number of electrode channels in the LFP data file

        Parameters
        ----------
        None

        Returns
        -------
        int
            Total number of electrode channels
        &#34;&#34;&#34;

        return self._record_info[&#39;No of channels&#39;]

    def get_timestamp_bytes(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each timestamp in the binary file

        Parameters
        ----------
        None

        Returns
        -------
        int
            Number of bytes to represent timestamps

        &#34;&#34;&#34;

        return self._record_info[&#39;Bytes per timestamp&#39;]

    def get_sampling_rate(self):
        &#34;&#34;&#34;
        Returns the sampling rate of spike waveforms

        Parameters
        ----------
        None

        Returns
        -------
        int
            Sampling rate for spike waveforms

        &#34;&#34;&#34;

        return self._record_info[&#39;Sampling rate&#39;]

    def get_bytes_per_sample(self):
        &#34;&#34;&#34;
        Returns the number of bytes to represent each LFP waveform sample

        Parameters
        ----------
        None

        Returns
        -------
        int
            Number of bytes to represent each sample of the LFP waveform

        &#34;&#34;&#34;

        return self._record_info[&#39;Bytes per sample&#39;]

    def get_fullscale_mv(self):
        &#34;&#34;&#34;
        Returns the fullscale value of the ADC in mV

        Parameters
        ----------
        None

        Returns
        -------
        int
            Fullscale ADC value in mV

        &#34;&#34;&#34;

        return self._record_info[&#39;ADC Fullscale mv&#39;]

    def get_recording_time(self):
        &#34;&#34;&#34;
        Returns the recording time in seconds

        Parameters
        ----------
        None

        Returns
        -------
        int
            Recording time in seconds
        &#34;&#34;&#34;

        return self.get_total_samples() / (self.get_sampling_rate())

    def load(self, filename=None, system=None):
        &#34;&#34;&#34;
        Loads LFP datasets

        Parameters
        ----------
        filename : str
            Name of the spike datafile
        system : str
            Recording system or format of the spike data file

        Returns
        -------
        None

        See also
        --------
        load_lfp_axona(), load_lfp_NLX(), load_lfp_NWB()

        &#34;&#34;&#34;
        if system is None:
            system = self._system
        else:
            self._system = system
        if filename is None:
            filename = self._filename
        else:
            self._filename = filename
        loader = getattr(self, &#39;load_lfp_&#39; + system)
        loader(filename)

    def add_spike(self, spike=None, **kwargs):
        &#34;&#34;&#34;
        Adds new spike node to current NLfp() object

        Parameters
        ----------
        spike : NSpikes
            NSPike object. If None, new object is created

        Returns
        -------
        `:obj:NSpike()`
            A new NSpike() object

        &#34;&#34;&#34;

        cls= kwargs.get(&#39;cls&#39;, None)
        if not inspect.isclass(cls):
            try:
                data_type = spike.get_type()
                if data_type == &#39;spike&#39;:
                    cls = spike.__class__
            except:
                 logging.error(&#39;Data type cannot be determined!&#39;)
        if inspect.isclass(cls):
             new_spike = self._add_node(cls, spike, &#39;spike&#39;, **kwargs)
             return new_spike
        else:
            logging.error(&#39;Cannot add the spike data!&#39;)


    def load_spike(self, names=&#39;all&#39;):
        &#34;&#34;&#34;
        Loads datasets of the spike nodes. Name of each node is used for obtaining the
        filenames

        Parameters
        ----------
        names : list of str
            Names of the nodes to load. If None, current NSpike() object is loaded

        Returns
        -------
        None

        &#34;&#34;&#34;


        if names == &#39;all&#39;:
            for spike in self._spikes:
                spike.load()
        else:
            logging.error(&#34;Spikes by name has yet to be implemented&#34;)
            # for name in names:
            #     spike = self.get_spikes_by_name(name)
            #     spike.load()

    def add_lfp(self, lfp=None, **kwargs):
        &#34;&#34;&#34;
        Adds new LFP node to current NLfp() object

        Parameters
        ----------
        lfp : NLfp
            NLfp object. If None, new object is created

        Returns
        -------
        `:obj:Nlfp`
            A new NLfp() object

        &#34;&#34;&#34;

        new_lfp = self._add_node(self.__class__, lfp, &#39;lfp&#39;, **kwargs)

        return new_lfp

    def load_lfp(self, names=None):
        &#34;&#34;&#34;
        Loads datasets of the LFP nodes. Name of each node is used for obtaining the
        filenames

        Parameters
        ----------
        names : list of str
            Names of the nodes to load. If `all`, all LFP nodes are loaded

        Returns
        -------
        None
        &#34;&#34;&#34;

        if names is None:
            self.load()
        elif names == &#39;all&#39;:
            for lfp in self._lfp:
                lfp.load()
        else:
            logging.error(&#34;Lfp by name has yet to be implemented&#34;)
            # for name in names:
            #     lfp = self.get_lfp_by_name(name)
            #     lfp.load()

    def spectrum(self, **kwargs):
        &#34;&#34;&#34;
        Analyses frequency spectrum of the LFP signal

        Parameters
        ----------
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        graph_data = oDict()

        Fs = self.get_sampling_rate()
        slc = kwargs.get(&#39;slice&#39;, None)
        if slc:
            lfp = self.get_samples()[slc]
        else:
            lfp = self.get_samples()

        window = kwargs.get(&#39;window&#39;, 1.0)
        window = sg.get_window(&#39;hann&#39;, int(window*Fs)) if isinstance(window, float)\
                or isinstance(window, int) else window

        win_sec = np.ceil(window.size/Fs)

        noverlap = kwargs.get(&#39;noverlap&#39;, 0.5*win_sec)
        noverlap = noverlap if noverlap &lt; win_sec else 0.5*win_sec
        noverlap = np.ceil(noverlap*Fs)

        nfft = kwargs.get(&#39;nfft&#39;, 2*Fs)
        nfft = np.power(2, int(np.ceil(np.log2(nfft))))

        ptype = kwargs.get(&#39;ptype&#39;, &#39;psd&#39;)
        ptype = &#39;spectrum&#39; if ptype == &#39;power&#39; else &#39;density&#39;

        prefilt = kwargs.get(&#39;prefilt&#39;, True)
        _filter = kwargs.get(&#39;filtset&#39;, [10, 1.5, 40, &#39;bandpass&#39;])

        fmax = kwargs.get(&#39;fmax&#39;, Fs/2)

        if prefilt:
            lfp = butter_filter(lfp, Fs, *_filter)

        tr = kwargs.get(&#39;tr&#39;, False)
        db = kwargs.get(&#39;db&#39;, False)
        if tr:
            f, t, Sxx = sg.spectrogram(lfp, fs=Fs, \
                    window=window, nperseg=window.size, noverlap=noverlap, nfft=nfft, \
                    detrend=&#39;constant&#39;, return_onesided=True, scaling=ptype)

            graph_data[&#39;t&#39;] = t
            graph_data[&#39;f&#39;] = f[find(f &lt;= fmax)]

            if db:
                Sxx = 10*np.log10(Sxx/np.amax(Sxx))
                Sxx = Sxx.flatten()
                Sxx[find(Sxx &lt; -40)] = -40
                Sxx = np.reshape(Sxx, [f.size, t.size])

#            graph_data[&#39;Sxx&#39;] = np.empty([find(f&lt;= fmax).size, t.size])
#            graph_data[&#39;Sxx&#39;] = np.array([Sxx[i, :] for i in find(f&lt;= fmax)])
            graph_data[&#39;Sxx&#39;] = Sxx[find(f &lt;= fmax), :]
        else:
            f, Pxx = sg.welch(lfp, fs=Fs, \
                    window=window, nperseg=window.size, noverlap=noverlap, nfft=nfft, \
                    detrend=&#39;constant&#39;, return_onesided=True, scaling=ptype)

            graph_data[&#39;f&#39;] = f[find(f &lt;= fmax)]

            if db:
                Pxx = 10*np.log10(Pxx/Pxx.max())
                Pxx[find(Pxx &lt; -40)] = -40
            graph_data[&#39;Pxx&#39;] = Pxx[find(f &lt;= fmax)]

        return graph_data

    def phase_dist(self, event_stamp, **kwargs):
        &#34;&#34;&#34;
        Analysis of spike to LFP phase distribution

        Parameters
        ----------
        evnet_stamp : ndarray
            Timestamps of the events of spiking activities for measring the phase
            distribution
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        _results= oDict()
        graph_data = oDict()

        cs = CircStat()

        lfp = self.get_samples()*1000
        Fs = self.get_sampling_rate()
        time = self.get_timestamp()

        # Input parameters
        bins = int(360/kwargs.get(&#39;binsize&#39;, 5))
        rbinsize = kwargs.get(&#39;rbinsize&#39;, 2) # raster binsize
        rbins = int(360/rbinsize)
        fwin = kwargs.get(&#39;fwin&#39;, [6, 12])
        pratio = kwargs.get(&#39;pratio&#39;, 0.2)
        aratio = kwargs.get(&#39;aratio&#39;, 0.15)

    # Filter
        fmax = fwin[1]
        fmin = fwin[0]
        _filter = [5, fmin, fmax, &#39;bandpass&#39;]
        _prefilt = kwargs.get(&#39;filtset&#39;, [10, 1.5, 40, &#39;bandpass&#39;])

        b_lfp = butter_filter(lfp, Fs, *_filter) # band LFP
        lfp = butter_filter(lfp, Fs, *_prefilt)

    # Measure phase
        hilb = sg.hilbert(b_lfp)
#        self.hilb = hilb
#        phase = np.remainder(np.angle(hilb, deg=True)+ 360, 360)
        phase = np.angle(hilb, deg=True)
        phase[phase &lt; 0] = phase[phase &lt; 0] + 360
        mag = np.abs(hilb)

        ephase = np.interp(event_stamp, time, phase)

        p2p = np.abs(np.max(lfp) - np.min(lfp))
        xline = 0.5* np.mean(mag) # cross line

        # Detection algo
        # zero cross
        mag1 = mag[0:-3]
        mag2 = mag[1:-2]
        mag3 = mag[2:-1]

        xind = np.union1d(find(np.logical_and(mag1 &lt; xline, mag2 &gt; xline)), \
                find(np.logical_and(np.logical_and(mag1 &lt; xline, mag2 == xline), mag3 &gt; xline)))

        # Ignore segments &lt;1/fmax
        i = 0
        rcount = np.empty([0,])
        bcount = np.empty([0, 0])

        phBins = np.arange(0, 360, 360/bins)
        rbins = np.arange(0, 360, 360/rbins)

        seg_count = 0
        while i &lt; len(xind)-1:
            k = i+1
            while time[xind[k]]- time[xind[i]] &lt; 1/fmin and k &lt; len(xind)-1:
                k += 1
#            print(time[xind[i]], time[xind[k]])
            s_lfp = lfp[xind[i]: xind[k]]
            s_p2p = np.abs(np.max(s_lfp)- np.min(s_lfp))

            if s_p2p &gt;= aratio*p2p:
                s_psd, f = fft_psd(s_lfp, Fs)
                if np.sum(s_psd[np.logical_and(f &gt;= fmin, f &lt;= fmax)]) &gt; pratio* np.sum(s_psd):
                    # Phase distribution
                    s_phase = ephase[np.logical_and(event_stamp &gt; time[xind[i]], event_stamp &lt;= time[xind[k]])]
#                    print(s_phase.shape, s_phase.shape)

                    if not s_phase.shape[0]:
                        pass
                    else:
                        seg_count += 1
                        cs.set_theta(s_phase)
                        temp_count = cs.circ_histogram(bins=rbinsize)
#                        temp_count = np.histogram(s_phase, bins=rbins, range=[0, 360])
                        temp_count = temp_count[0]
                        if not rcount.size:
                            rcount = temp_count
                        else:
                            rcount = np.append(rcount, temp_count)

                        temp_count = np.histogram(s_phase, bins=bins, range=[0, 360])
                        temp_count = np.resize(temp_count[0], [1, bins])
                        if not len(bcount):
                            bcount = temp_count
                        else:
                            bcount = np.append(bcount, temp_count, axis=0)
            i = k

        rcount = rcount.reshape([seg_count, rbins.size])

        phCount = np.sum(bcount, axis=0)

        cs.set_rho(phCount)
        cs.set_theta(phBins)

        cs.calc_stat()
        result = cs.get_result()
        meanTheta = result[&#39;meanTheta&#39;]*np.pi/180

        _results[&#39;LFP Spike Mean Phase&#39;]= result[&#39;meanTheta&#39;]
        _results[&#39;LFP Spike Mean Phase Count&#39;]= result[&#39;meanRho&#39;]
        _results[&#39;LFP Spike Phase Res Vect&#39;]= result[&#39;resultant&#39;]

        graph_data[&#39;meanTheta&#39;] = meanTheta
        graph_data[&#39;phCount&#39;] = phCount
        graph_data[&#39;phBins&#39;] = phBins
        graph_data[&#39;raster&#39;] = rcount
        graph_data[&#39;rasterbins&#39;] = rbins

        self.update_result(_results)

        return graph_data

    def phase_at_events(self, event_stamps, **kwargs):
        &#34;&#34;&#34;
        Phase based on times.

        Parameters
        ----------
        event_stamps : array
            an array of event times
        **kwargs:
            keyword arguments

        Returns
        -------
            (array)
            Phase values for each position
        &#34;&#34;&#34;
        lfp = self.get_samples() * 1000
        Fs = self.get_sampling_rate()
        time = self.get_timestamp()

        # Input parameters
        fwin = kwargs.get(&#39;fwin&#39;, [6, 12])

        # Filter
        fmax = fwin[1]
        fmin = fwin[0]
        _filter = [5, fmin, fmax, &#39;bandpass&#39;]
        _prefilt = kwargs.get(&#39;filtset&#39;, [10, 1.5, 40, &#39;bandpass&#39;])

        b_lfp = butter_filter(lfp, Fs, *_filter)  # band LFP
        lfp = butter_filter(lfp, Fs, *_prefilt)

        # Measure phase
        hilb = sg.hilbert(b_lfp)
        phase = np.angle(hilb, deg=True)
        phase[phase &lt; 0] = phase[phase &lt; 0] + 360

        ephase = np.interp(event_stamps, time, phase)

        return ephase

    def plv(self, event_stamp, **kwargs):
        &#34;&#34;&#34;
        Calculates phase-locking value of the spike train to underlying LFP signal.

        When &#39;mode&#39;= None in the inpput kwargs, it calculates the PLV and SFC over
        the entire spike-train.

        If &#39;mode&#39;= &#39;bs&#39;, it bootstraps the spike-timestamps
        and calculates the locking values for each set of new spike timestamps.

        If &#39;mode&#39;= &#39;tr&#39;, a time-resilved phase-locking analysis is performed where
        the LFP signal is split into overlapped segments for each calculation.

        Parameters
        ----------
        evnet_stamp : ndarray
            Timestamps of the events or the spiking activities for measuring the phase
            locking
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;
        graph_data = oDict()

        lfp = self.get_samples()*1000
        Fs = self.get_sampling_rate()
        time = self.get_timestamp()

        window = np.array(kwargs.get(&#39;window&#39;, [-0.5, 0.5]))
        win = np.ceil(window*Fs).astype(int)
        win = np.arange(win[0], win[1])
        slep_win = sg.hann(win.size, False)

        nfft = kwargs.get(&#39;nfft&#39;, 1024)
        mode = kwargs.get(&#39;mode&#39;, None) # None, &#39;bs&#39;, &#39;tr&#39; bs=bootstrp, tr=time-resolved
        fwin = kwargs.get(&#39;fwin&#39;, [])

        xf = np.arange(0, Fs, Fs/nfft)
        f = xf[0: int(nfft/2)+ 1]

        ind = np.arange(f.size) if len(fwin) == 0 else find(np.logical_and(f &gt;= fwin[0], f &lt;= fwin[1]))

        if mode == &#39;bs&#39;:
            nsample = kwargs.get(&#39;nsample&#39;, 50)
            nrep = kwargs.get(&#39;nrep&#39;, 500)

            STA = np.empty([nrep, win.size])
            fSTA = np.empty([nrep, ind.size])
            STP = np.empty([nrep, ind.size])
            SFC = np.empty([nrep, ind.size])
            PLV = np.empty([nrep, ind.size])

            for i in np.arange(nrep):
                data = self.plv(np.random.choice(event_stamp, nsample, False), \
                        window=window, nfft=nfft, mode=None, fwin=fwin)
                t = data[&#39;t&#39;]
                STA[i, :] = data[&#39;STA&#39;]
                fSTA[i, :] = data[&#39;fSTA&#39;]
                STP[i, :] = data[&#39;STP&#39;]
                SFC[i, :] = data[&#39;SFC&#39;]
                PLV[i, :] = data[&#39;PLV&#39;]

            graph_data[&#39;t&#39;] = t
            graph_data[&#39;f&#39;] = f[ind]
            graph_data[&#39;STAm&#39;] = STA.mean(0)
            graph_data[&#39;fSTAm&#39;] = fSTA.mean(0)
            graph_data[&#39;STPm&#39;] = STP.mean(0)
            graph_data[&#39;SFCm&#39;] = SFC.mean(0)
            graph_data[&#39;PLVm&#39;] = PLV.mean(0)

            graph_data[&#39;STAe&#39;] = stats.sem(STA, 0)
            graph_data[&#39;fSTAe&#39;] = stats.sem(fSTA, 0)
            graph_data[&#39;STPe&#39;] = stats.sem(STP, 0)
            graph_data[&#39;SFCe&#39;] = stats.sem(SFC, 0)
            graph_data[&#39;PLVe&#39;] = stats.sem(PLV, 0)

        elif mode == &#39;tr&#39;:
            nsample = kwargs.get(&#39;nsample&#39;, None)

            slide = kwargs.get(&#39;slide&#39;, 25) # in ms
            slide = slide/1000 # convert to sec

            offset = np.arange(window[0], window[-1], slide)
            nwin = offset.size

            fSTA = np.empty([nwin, ind.size])
            STP = np.empty([nwin, ind.size])
            SFC = np.empty([nwin, ind.size])
            PLV = np.empty([nwin, ind.size])

            if nsample is None or nsample &gt; event_stamp.size:
                stamp = event_stamp
            else:
                stamp = np.random.choice(event_stamp, nsample, False)

            for i in np.arange(nwin):
                data = self.plv(stamp + offset[i], \
                        nfft=nfft, mode=None, fwin=fwin, window=window)
                t = data[&#39;t&#39;]
                fSTA[i, :] = data[&#39;fSTA&#39;]
                STP[i, :] = data[&#39;STP&#39;]
                SFC[i, :] = data[&#39;SFC&#39;]
                PLV[i, :] = data[&#39;PLV&#39;]

            graph_data[&#39;offset&#39;] = offset
            graph_data[&#39;f&#39;] = f[ind]
            graph_data[&#39;fSTA&#39;] = fSTA.transpose()
            graph_data[&#39;STP&#39;] = STP.transpose()
            graph_data[&#39;SFC&#39;] = SFC.transpose()
            graph_data[&#39;PLV&#39;] = PLV.transpose()

        elif mode is None:
            center = time.searchsorted(event_stamp)
            # Keep windows within data
            center = np.array([center[i] for i in range(0, len(event_stamp)) \
                if center[i] + win[0] &gt;= 0 and center[i] + win[-1] &lt;= time.size])

            sta_data = self.event_trig_average(event_stamp, **kwargs)
            STA = sta_data[&#39;ETA&#39;]

            fSTA = fft(np.multiply(STA, slep_win), nfft)

            fSTA = np.absolute(fSTA[0: int(nfft/2)+ 1])**2/nfft**2
            fSTA[1:-1] = 2*fSTA[1:-1]

            fLFP = np.array([fft(np.multiply(lfp[x+ win], slep_win), nfft) \
                    for x in center])

            STP = np.absolute(fLFP[:, 0: int(nfft/2)+ 1])**2/nfft**2
            STP[:, 1:-1] = 2*STP[:, 1:-1]
            STP = STP.mean(0)

            SFC = np.divide(fSTA, STP)*100

            PLV = np.copy(fLFP)

            # Normalize
            PLV = np.divide(PLV, np.absolute(PLV))
            PLV[np.isnan(PLV)] = 0

            PLV = np.absolute(PLV.mean(0))[0: int(nfft/2)+ 1]
            PLV[1:-1] = 2*PLV[1:-1]

            graph_data[&#39;t&#39;] = sta_data[&#39;t&#39;]
            graph_data[&#39;f&#39;] = f[ind]
            graph_data[&#39;STA&#39;] = STA
            graph_data[&#39;fSTA&#39;] = fSTA[ind]
            graph_data[&#39;STP&#39;] = STP[ind]
            graph_data[&#39;SFC&#39;] = SFC[ind]
            graph_data[&#39;PLV&#39;] = PLV[ind]

        return graph_data

    def event_trig_average(self, event_stamp=None, **kwargs):
        &#34;&#34;&#34;
        Averaging event-triggered LFP signals

        Parameters
        ----------
        event_stamp : ndarray
            Timestamps of the events or the spiking activities for measuring the
            event triggered average of the LFP signal
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        graph_data = oDict()
        window = np.array(kwargs.get(&#39;window&#39;, [-0.5, 0.5]))

        if event_stamp is None:
            spike = kwargs.get(&#39;spike&#39;, None)

            try:
                data_type = spike.get_type()
            except:
                logging.error(&#39;The data type of the addes object cannot be determined!&#39;)

            if data_type == &#39;spike&#39;:
                event_stamp = spike.get_unit_stamp()
            elif spike in self.get_spike_names():
                event_stamp = self.get_spike(spike).get_unit_stamp()

        if event_stamp is None:
            logging.error(&#39;No valid event timestamp or spike is provided&#39;)
        else:
            lfp = self.get_samples()*1000
            Fs = self.get_sampling_rate()
            time = self.get_timestamp()
            center = time.searchsorted(event_stamp, side=&#39;left&#39;)
            win = np.ceil(window*Fs).astype(int)
            win = np.arange(win[0], win[1])

            # Keep windows within data
            center = np.array([center[i] for i in range(0, len(event_stamp)) \
                if center[i]+ win[0] &gt;= 0 and center[i]+ win[-1] &lt;= time.size])

            eta = reduce(lambda y, x: y+ lfp[x+ win], center)
            eta = eta/center.size

            graph_data[&#39;t&#39;] = win/Fs
            graph_data[&#39;ETA&#39;] = eta
            graph_data[&#39;center&#39;] = center

        return graph_data

    def spike_lfp_causality(self, spike=None, **kwargs):
        &#34;&#34;&#34;
        (Not implemented yet)

        Analyses spike to underlying LFP causality

        Parameters
        ----------
        spike : NSpike
            Spike dataset which is used for the causality analysis
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Should return graphical data of the analysis. The function is not
            implemented yet.

        &#34;&#34;&#34;

        pass

    def subsample(self, sample_range=None):
        &#34;&#34;&#34;
        Extract a time range from the lfp.

        Parameters
        ----------
        sample_range : tuple
            the time in seconds to extract from the lfp

        Returns
        -------
        NLfp
            subsampled version of initial lfp object
        &#34;&#34;&#34;
        in_range = sample_range
        sample_rate = self.get_sampling_rate()
        if in_range is None:
            length = int(self.get_duration() * sample_rate)
            if (length != self.get_total_samples()):
                logging.warning(
                    &#34;Unequal calculated and recorded total lfp samples&#34; +
                    &#34;Calculated {} and recorded {}&#34;.format(
                        length, self.get_total_samples()))
            return self
        else:
            new_lfp = deepcopy(self)
            lfp_samples = self.get_samples()[
                int(sample_rate * in_range[0]):int(sample_rate * in_range[1])]
            lfp_times = self.get_timestamp()[
                int(sample_rate * in_range[0]):int(sample_rate * in_range[1])]
            new_lfp._set_samples(lfp_samples)
            new_lfp._set_timestamp(lfp_times)
            new_lfp._set_total_samples(len(lfp_samples))
            new_lfp._set_duration(in_range[1] - in_range[0])
            return new_lfp

    def sharp_wave_ripples(self, in_range=None, **kwargs):
        &#34;&#34;&#34;
        Detect SWR events in the lfp, optionally in a given range

        Parameters
        ----------
        in_range : tuple
            A range in seconds

        kwargs
        ------
        swr_lower : float
            Lower band in hz
        swr_upper : float
            Upper band in hz
        rms_window_size_ms : int
            Size of the rms window in ms
        percentile : float
            The percentile threshold for a peak

        Returns
        -------
        dict
            lfp times, lfp samples, swr times, lfp sample rate

        &#34;&#34;&#34;
        swr_lower = kwargs.get(&#34;swr_lower&#34;, 100)
        swr_higher = kwargs.get(&#34;swr_upper&#34;, 250)
        rms_window_size_ms = kwargs.get(&#34;rms_window_size_ms&#34;, 7)
        percentile = kwargs.get(&#34;peak_percentile&#34;, 99.5)

        lfp = self.subsample(in_range)
        sample_rate = lfp.get_sampling_rate()
        # Estimate SWR events
        filtered_lfp = butter_filter(
            lfp.get_samples(), sample_rate, 10,
            swr_lower, swr_higher, &#39;bandpass&#39;)
        rms_window_size = floor((rms_window_size_ms / 1000) * sample_rate)
        rms_envelope = window_rms(filtered_lfp, rms_window_size, mode=&#34;same&#34;)
        p_val = np.percentile(rms_envelope, percentile)
        _, peaks = find_peaks(rms_envelope, thresh=p_val)
        peaks = lfp.get_timestamp()[0] + (peaks / sample_rate)

        &#34;&#34;&#34;
        Alternative way to get SWR
        #rms_envelope = distinct_window_rms(filtered_lfp, rms_window_size)
        #peaks = (
        # longest_sleep_period[0] + peaks * rms_window_size) / sample_rate
        &#34;&#34;&#34;

        return {
            &#34;lfp times&#34;: lfp.get_timestamp(),
            &#34;lfp samples&#34;: filtered_lfp,
            &#34;swr times&#34;: peaks, &#34;lfp sample rate&#34;: sample_rate}

    def bandpower(self, band, **kwargs):
        &#34;&#34;&#34;Compute the average power of the signal x in a specific frequency band.

        Modified from excellent article at
        https://raphaelvallat.com/bandpower.html

        Parameters
        ----------
        band : list
        Lower and upper frequencies of the band of interest.

        kwargs:
            method : string
                Periodogram method: &#39;welch&#39;
            window_sec : float
                Length of each window in seconds.
                If None, window_sec = (1 / min(band)) * 2.
            band_total : bool
                Whether to band the total power
                Default False
            total_band: List
                low and high frequency values for the filter
                Default [1.5, 40]

        Returns
        ------
        bp : Dict
            &#34;bandpower&#34;, &#34;total_power&#34; and &#34;relative_power&#34;.
        &#34;&#34;&#34;
        from scipy.signal import welch
        from scipy.integrate import simps

        band = np.asarray(band)
        low, high = band
        method = kwargs.get(&#34;method&#34;, &#34;welch&#34;)
        window_sec = kwargs.get(&#34;window_sec&#34;, 2 / (low + 0.000001))
        sf = self.get_sampling_rate()
        lfp_samples = self.get_samples()

        band_total = kwargs.get(&#39;band_total&#39;, False)
        _filter = kwargs.get(&#39;total_band&#39;, [1.5, 40])

        # if prefilt:
        #     lfp_samples = butter_filter(lfp_samples, sf, *_filter)
        # Compute the modified periodogram (Welch)
        if method == &#39;welch&#39;:
            nperseg = int(window_sec * sf)
            freqs, psd = welch(lfp_samples, sf, nperseg=nperseg)

        # The multaper method is more accurate but we will not use it
        # Welch&#39;s method is still very good
        # See MNE for the multitaper method
        # from mne.time_frequency import psd_array_multitaper
        # elif method == &#39;multitaper&#39;:
        #     psd, freqs = psd_array_multitaper(lfp_samples, sf, adaptive=True,
        #                                     normalization=&#39;full&#39;, verbose=0)

        # Frequency resolution
        freq_res = freqs[1] - freqs[0]

        # Find index of band in frequency vector
        idx_band = np.logical_and(freqs &gt;= low, freqs &lt;= high)

        # Integral approximation of the spectrum using parabola (Simpson&#39;s rule)
        bp = simps(psd[idx_band], dx=freq_res)

        if band_total:
            idx_band = np.logical_and(
                freqs &gt;= _filter[0],
                freqs &lt;= _filter[1])
            tp = simps(psd[idx_band], dx=freq_res)
        else:
            tp = simps(psd, dx=freq_res)

        output = {
            &#34;bandpower&#34;: bp,
            &#34;total_power&#34;: tp,
            &#34;relative_power&#34;: bp/tp}
        return output

    def bandpower_ratio(self, first_band, second_band, win_sec, **kwargs):
        &#34;&#34;&#34;
        Calculate the ratio in power between two bandpass filtered signals.

        Note that common ranges are:
        delta (1.5–4 Hz), theta (5-11 Hz)

        Parameters
        ----------
        first_band - 1d array
            lower and upper bands
        second_band - 1d array
            lower and upper bands
        win_sec - float
            length of the windows to bin lfp into in seconds.
            recommend 4 for eg.
        kwargs:
            first_name - str name of band 1, default &#34;Band 1&#34;
            second_name - str name of band 2, default &#34;Band 2&#34;

        Returns
        -------
        float - the ratio between the power signals.

        See also
        --------
        nc_lfp.NLfp().bandpower()
        &#34;&#34;&#34;

        _results = oDict()
        name1 = kwargs.get(&#34;first_name&#34;, &#34;Band 1&#34;)
        name2 = kwargs.get(&#34;second_name&#34;, &#34;Band 2&#34;)
        if &#34;window_sec&#34; not in kwargs:
            kwargs[&#34;window_sec&#34;] = win_sec

        b1 = self.bandpower(first_band, **kwargs)
        b2 = self.bandpower(second_band, **kwargs)
        if b1[&#34;total_power&#34;] != b2[&#34;total_power&#34;]:
            logging.error(
                &#34;Differing total power in lfp bandpower ratio calculations&#34;)
        bp = b1[&#34;bandpower&#34;] / b2[&#34;bandpower&#34;]
        key1 = name1 + &#34; Power&#34;
        key2 = name2 + &#34; Power&#34;
        key3 = name1 + &#34; &#34; + name2 + &#34; Power Ratio&#34;
        _results[key1] = b1[&#34;bandpower&#34;]
        _results[key1 + &#34; (Relative)&#34;] = b1[&#34;relative_power&#34;]
        _results[key2] = b2[&#34;bandpower&#34;]
        _results[key2 + &#34; (Relative)&#34;] = b2[&#34;relative_power&#34;]
        _results[key3] = bp
        _results[&#34;Total Power&#34;] = b1[&#34;total_power&#34;]
        self.update_result(_results)
        return bp

    def save_to_hdf5(self, file_name=None, system=None):
        &#34;&#34;&#34;
        Stores NLfp() object to HDF5 file

        Parameters
        ----------
        file_name : str
            Full file directory for the lfp data
        system : str
            Recoring system or data format

        Returns
        -------
        None

        Also see
        --------
        nc_hdf.Nhdf().save_lfp()

        &#34;&#34;&#34;

        hdf = Nhdf()
        if file_name and system:
            if os.path.exists(file_name):
                self.set_filename(file_name)
                self.set_system(system)
                self.load()
            else:
                logging.error(&#39;Specified file cannot be found!&#39;)

        hdf.save_lfp(lfp=self)
        hdf.close()

    def load_lfp_NWB(self, file_name):
        &#34;&#34;&#34;
        Decodes LFP data from NWB (HDF5) file format

        Parameters
        ----------
        file_name : str
            Full file directory for the lfp data

        Returns
        -------
        None

        &#34;&#34;&#34;

        file_name, path = file_name.split(&#39;+&#39;)
        if os.path.exists(file_name):
            hdf = Nhdf()
            hdf.set_filename(file_name)

            _record_info = {}

            if path in hdf.f:
                g = hdf.f[path]
            elif &#39;/processing/Neural Continuous/LFP/&#39;+ path in hdf.f:
                path = &#39;/processing/Neural Continuous/LFP/&#39;+ path
                g = hdf.f[path]
            else:
                logging.error(&#39;Specified path does not exist!&#39;)

            for key, value in g.attrs.items():
                _record_info[key] = value

            self.set_record_info(_record_info)

            self._set_samples(hdf.get_dataset(group=g, name=&#39;data&#39;))
            self._set_timestamp(hdf.get_dataset(group=g, name=&#39;timestamps&#39;))
            self._set_total_samples(hdf.get_dataset(group=g, name=&#39;num_samples&#39;))

            hdf.close()
        else:
            logging.error(file_name + &#39; does not exist!&#39;)

    def load_lfp_Axona(self, file_name):
        &#34;&#34;&#34;
        Decodes LFP data from Axona file format

        Parameters
        ----------
        file_name : str
            Full file directory for the lfp data

        Returns
        -------
        None

        &#34;&#34;&#34;

        words = file_name.split(sep=os.sep)
        file_directory = os.sep.join(words[0:-1])
        file_tag, file_extension = words[-1].split(&#39;.&#39;)
        set_file = file_directory + os.sep + file_tag + &#39;.set&#39;

        self._set_data_source(file_name)
        self._set_source_format(&#39;Axona&#39;)

        if os.path.isfile(file_name):
            with open(file_name, &#39;rb&#39;) as f:
                while True:
                    line = f.readline()
                    try:
                        line = line.decode(&#39;latin-1&#39;)
                    except:
                        break

                    if line == &#39;&#39;:
                        break
                    if line.startswith(&#39;trial_date&#39;):
                        self._set_date(&#39; &#39;.join(line.replace(&#39;,&#39;, &#39; &#39;).split()[1:]))
                    if line.startswith(&#39;trial_time&#39;):
                        self._set_time(line.split()[1])
                    if line.startswith(&#39;experimenter&#39;):
                        self._set_experiemnter(&#39; &#39;.join(line.split()[1:]))
                    if line.startswith(&#39;comments&#39;):
                        self._set_comments(&#39; &#39;.join(line.split()[1:]))
                    if line.startswith(&#39;duration&#39;):
                        self._set_duration(float(&#39;&#39;.join(line.split()[1:])))
                    if line.startswith(&#39;sw_version&#39;):
                        self._set_file_version(line.split()[1])
                    if line.startswith(&#39;num_chans&#39;):
                        self._set_total_channel(int(&#39;&#39;.join(line.split()[1:])))
                    if line.startswith(&#39;sample_rate&#39;):
                        self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                    if line.startswith(&#39;bytes_per_sample&#39;):
                        self._set_bytes_per_sample(int(&#39;&#39;.join(line.split()[1:])))
                    if line.startswith(&#39;num_&#39;+ file_extension[:3].upper() + &#39;_samples&#39;):
                        self._set_total_samples(int(&#39;&#39;.join(line.split()[1:])))
                    if line.startswith(&#34;data_start&#34;):
                        break

                num_samples = self.get_total_samples()
                bytes_per_sample = self.get_bytes_per_sample()

                f.seek(0, 0)
                header_offset = []
                while True:
                    try:
                        buff = f.read(10).decode(&#39;UTF-8&#39;)
                    except:
                        break
                    if buff == &#39;data_start&#39;:
                        header_offset = f.tell()
                        break
                    else:
                        f.seek(-9, 1)

                eeg_ID = re.findall(r&#39;\d+&#39;, file_extension)
                self.set_file_tag(1 if not eeg_ID else int(eeg_ID[0]))
                max_ADC_count = 2**(8*bytes_per_sample-1)-1
                max_byte_value = 2**(8*bytes_per_sample)

                with open(set_file, &#39;r&#39;, encoding=&#39;latin-1&#39;) as f_set:
                    lines = f_set.readlines()
                    channel_lines = dict([tuple(map(int, re.findall(r&#39;\d+.\d+|\d+&#39;, line)[0].split()))\
                                for line in lines if line.startswith(&#39;EEG_ch_&#39;)])
                    channel_id = channel_lines[self.get_file_tag()]
                    self.set_channel_id(channel_id)

                    gain_lines = dict([tuple(map(int, re.findall(r&#39;\d+.\d+|\d+&#39;, line)[0].split()))\
                            for line in lines if &#39;gain_ch_&#39; in line])
                    gain = gain_lines[channel_id-1]

                    for line in lines:
                        if line.startswith(&#39;ADC_fullscale_mv&#39;):
                            self._set_fullscale_mv(int(re.findall(r&#39;\d+.\d+|d+&#39;, line)[0]))
                            break
                    AD_bit_uvolt = 2*self.get_fullscale_mv()/ \
                                    (gain*np.power(2, 8*bytes_per_sample))

                record_size = bytes_per_sample
                sample_le = 256**(np.arange(0, bytes_per_sample, 1))

                if not header_offset:
                    print(&#39;Error: data_start marker not found!&#39;)
                else:
                    f.seek(header_offset, 0)
                    byte_buffer = np.fromfile(f, dtype=&#39;uint8&#39;)
                    len_bytebuffer = len(byte_buffer)
                    end_offset = len(&#39;\r\ndata_end\r&#39;)
                    lfp_wave = np.zeros([num_samples, ], dtype=np.float64)
                    for k in np.arange(0, bytes_per_sample, 1):
                        byte_offset = k
                        sample_value = (sample_le[k]* byte_buffer[byte_offset \
                                    :byte_offset+ len_bytebuffer- end_offset- record_size\
                                    :record_size])
                        if sample_value.size &lt; num_samples:
                            sample_value = np.append(sample_value, np.zeros([num_samples-sample_value.size,]))
                        sample_value = sample_value.astype(np.float64, casting=&#39;unsafe&#39;, copy=False)
                        np.add(lfp_wave, sample_value, out=lfp_wave)
                    np.putmask(lfp_wave, lfp_wave &gt; max_ADC_count, lfp_wave- max_byte_value)

                    self._set_samples(lfp_wave*AD_bit_uvolt)
                    self._set_timestamp(np.arange(0, num_samples, 1)/self.get_sampling_rate())

        else:
            logging.error(
                &#34;No lfp file found for file {}&#34;.format(file_name))

    def load_lfp_Neuralynx(self, file_name):
        &#34;&#34;&#34;
        Decodes LFP data from Neuralynx file format

        Parameters
        ----------
        file_name : str
            Full file directory for the lfp data

        Returns
        -------
        None

        &#34;&#34;&#34;

        self._set_data_source(file_name)
        self._set_source_format(&#39;Neuralynx&#39;)

        # Format description for the NLX file:

        resamp_freq = 250 # NeuroChaT subsamples the original recording from 32000 to 250

        header_offset = 16*1024 # fixed for NLX files

        bytes_per_timestamp = 8
        bytes_chan_no = 4
        bytes_sample_freq = 4
        bytes_num_valid_samples = 4
        bytes_per_sample = 2
        samples_per_record = 512

        max_byte_value = np.power(2, bytes_per_sample*8)
        max_ADC_count = np.power(2, bytes_per_sample*8- 1)-1
        AD_bit_uvolt = 10**-6

        self._set_bytes_per_sample(bytes_per_sample)

        record_size = None
        with open(file_name, &#39;rb&#39;) as f:
            while True:
                line = f.readline()
                try:
                    line = line.decode(&#39;UTF-8&#39;)
                except:
                    break

                if line == &#39;&#39;:
                    break
                if &#39;SamplingFrequency&#39; in line:
                    self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))) # We are subsampling from the blocks of 512 samples per record
                if &#39;RecordSize&#39; in line:
                    record_size = int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;Time Opened&#39; in line:
                    self._set_date(re.search(r&#39;\d+/\d+/\d+&#39;, line).group())
                    self._set_time(re.search(r&#39;\d+:\d+:\d+&#39;, line).group())
                if &#39;FileVersion&#39; in line:
                    self._set_file_version(line.split()[1])
                if &#39;ADMaxValue&#39; in line:
                    max_ADC_count = float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
                if &#39;ADBitVolts&#39; in line:
                    AD_bit_uvolt = float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))*(10**6)

            self._set_fullscale_mv(max_byte_value*AD_bit_uvolt/2) # gain = 1 assumed to keep in similarity to Axona

            if not record_size:
                record_size = bytes_per_timestamp+ \
                             bytes_chan_no+ \
                             bytes_sample_freq+ \
                             bytes_num_valid_samples+ \
                             bytes_per_sample*samples_per_record

            time_offset = 0
            sample_freq_offset = bytes_per_timestamp+ bytes_chan_no
            num_valid_samples_offset = sample_freq_offset+ bytes_sample_freq
            sample_offset = num_valid_samples_offset+ bytes_num_valid_samples
            f.seek(0, 2)
            num_samples = int((f.tell()- header_offset)/record_size)

            f.seek(header_offset, 0)
            time = np.array([])
            lfp_wave = np.array([])
            sample_le = 256**(np.arange(0, bytes_per_sample, 1))
            for _ in np.arange(num_samples):
                sample_bytes = np.fromfile(f, dtype=&#39;uint8&#39;, count=record_size)
                block_start = int.from_bytes(sample_bytes[time_offset+ \
                                np.arange(bytes_per_timestamp)], byteorder=&#39;little&#39;, signed=False)/10**6
                valid_samples = int.from_bytes(sample_bytes[num_valid_samples_offset+ \
                                np.arange(bytes_num_valid_samples)], byteorder=&#39;little&#39;, signed=False)
                sampling_freq = int.from_bytes(sample_bytes[sample_freq_offset+ \
                                np.arange(bytes_sample_freq)], byteorder=&#39;little&#39;, signed=False)

                wave_bytes = sample_bytes[sample_offset+ np.arange(valid_samples* bytes_per_sample)]\
                                .reshape([valid_samples, bytes_per_sample])
                block_wave = np.dot(wave_bytes, sample_le)
                #    for k in np.arange(valid_samples):
                #        block_wave[k] = int.from_bytes(sample_bytes[sample_offset+ k*bytes_per_sample+ \
                #                    np.arange(bytes_per_sample)], byteorder=&#39;little&#39;, signed=False)
                np.putmask(block_wave, block_wave &gt; max_ADC_count, block_wave - max_byte_value)
                block_wave = block_wave*AD_bit_uvolt
                block_time = block_start +  np.arange(valid_samples)/ sampling_freq
                interp_time = np.arange(block_start, block_time[-1], 1/resamp_freq)
                interp_wave = np.interp(interp_time, block_time, block_wave)
                time = np.append(time, interp_time)
                lfp_wave = np.append(lfp_wave, interp_wave)
            time -= time.min()
            self._set_samples(lfp_wave)
            self._set_total_samples(lfp_wave.size)
            self._set_timestamp(time)
            self._set_sampling_rate(resamp_freq)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="neurochat.nc_base.NBase" href="nc_base.html#neurochat.nc_base.NBase">NBase</a></li>
<li><a title="neurochat.nc_base.NAbstract" href="nc_base.html#neurochat.nc_base.NAbstract">NAbstract</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="neurochat.nc_lfp.NLfp.add_lfp"><code class="name flex">
<span>def <span class="ident">add_lfp</span></span>(<span>self, lfp=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Adds new LFP node to current NLfp() object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>lfp</code></strong> :&ensp;<a title="neurochat.nc_lfp.NLfp" href="#neurochat.nc_lfp.NLfp"><code>NLfp</code></a></dt>
<dd>NLfp object. If None, new object is created</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>:obj:Nlfp</code>
A new NLfp() object</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_lfp(self, lfp=None, **kwargs):
    &#34;&#34;&#34;
    Adds new LFP node to current NLfp() object

    Parameters
    ----------
    lfp : NLfp
        NLfp object. If None, new object is created

    Returns
    -------
    `:obj:Nlfp`
        A new NLfp() object

    &#34;&#34;&#34;

    new_lfp = self._add_node(self.__class__, lfp, &#39;lfp&#39;, **kwargs)

    return new_lfp</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.add_spike"><code class="name flex">
<span>def <span class="ident">add_spike</span></span>(<span>self, spike=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Adds new spike node to current NLfp() object</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spike</code></strong> :&ensp;<code>NSpikes</code></dt>
<dd>NSPike object. If None, new object is created</dd>
</dl>
<h2 id="returns">Returns</h2>
<p><code>:obj:NSpike()</code>
A new NSpike() object</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def add_spike(self, spike=None, **kwargs):
    &#34;&#34;&#34;
    Adds new spike node to current NLfp() object

    Parameters
    ----------
    spike : NSpikes
        NSPike object. If None, new object is created

    Returns
    -------
    `:obj:NSpike()`
        A new NSpike() object

    &#34;&#34;&#34;

    cls= kwargs.get(&#39;cls&#39;, None)
    if not inspect.isclass(cls):
        try:
            data_type = spike.get_type()
            if data_type == &#39;spike&#39;:
                cls = spike.__class__
        except:
             logging.error(&#39;Data type cannot be determined!&#39;)
    if inspect.isclass(cls):
         new_spike = self._add_node(cls, spike, &#39;spike&#39;, **kwargs)
         return new_spike
    else:
        logging.error(&#39;Cannot add the spike data!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.bandpower"><code class="name flex">
<span>def <span class="ident">bandpower</span></span>(<span>self, band, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Compute the average power of the signal x in a specific frequency band.</p>
<p>Modified from excellent article at
<a href="https://raphaelvallat.com/bandpower.html">https://raphaelvallat.com/bandpower.html</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>band</code></strong> :&ensp;<code>list</code></dt>
<dd>&nbsp;</dd>
</dl>
<p>Lower and upper frequencies of the band of interest.</p>
<p>kwargs:
method : string
Periodogram method: 'welch'
window_sec : float
Length of each window in seconds.
If None, window_sec = (1 / min(band)) * 2.
band_total : bool
Whether to band the total power
Default False
total_band: List
low and high frequency values for the filter
Default [1.5, 40]</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>bp</code></strong> :&ensp;<code>Dict</code></dt>
<dd>"bandpower", "total_power" and "relative_power".</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def bandpower(self, band, **kwargs):
    &#34;&#34;&#34;Compute the average power of the signal x in a specific frequency band.

    Modified from excellent article at
    https://raphaelvallat.com/bandpower.html

    Parameters
    ----------
    band : list
    Lower and upper frequencies of the band of interest.

    kwargs:
        method : string
            Periodogram method: &#39;welch&#39;
        window_sec : float
            Length of each window in seconds.
            If None, window_sec = (1 / min(band)) * 2.
        band_total : bool
            Whether to band the total power
            Default False
        total_band: List
            low and high frequency values for the filter
            Default [1.5, 40]

    Returns
    ------
    bp : Dict
        &#34;bandpower&#34;, &#34;total_power&#34; and &#34;relative_power&#34;.
    &#34;&#34;&#34;
    from scipy.signal import welch
    from scipy.integrate import simps

    band = np.asarray(band)
    low, high = band
    method = kwargs.get(&#34;method&#34;, &#34;welch&#34;)
    window_sec = kwargs.get(&#34;window_sec&#34;, 2 / (low + 0.000001))
    sf = self.get_sampling_rate()
    lfp_samples = self.get_samples()

    band_total = kwargs.get(&#39;band_total&#39;, False)
    _filter = kwargs.get(&#39;total_band&#39;, [1.5, 40])

    # if prefilt:
    #     lfp_samples = butter_filter(lfp_samples, sf, *_filter)
    # Compute the modified periodogram (Welch)
    if method == &#39;welch&#39;:
        nperseg = int(window_sec * sf)
        freqs, psd = welch(lfp_samples, sf, nperseg=nperseg)

    # The multaper method is more accurate but we will not use it
    # Welch&#39;s method is still very good
    # See MNE for the multitaper method
    # from mne.time_frequency import psd_array_multitaper
    # elif method == &#39;multitaper&#39;:
    #     psd, freqs = psd_array_multitaper(lfp_samples, sf, adaptive=True,
    #                                     normalization=&#39;full&#39;, verbose=0)

    # Frequency resolution
    freq_res = freqs[1] - freqs[0]

    # Find index of band in frequency vector
    idx_band = np.logical_and(freqs &gt;= low, freqs &lt;= high)

    # Integral approximation of the spectrum using parabola (Simpson&#39;s rule)
    bp = simps(psd[idx_band], dx=freq_res)

    if band_total:
        idx_band = np.logical_and(
            freqs &gt;= _filter[0],
            freqs &lt;= _filter[1])
        tp = simps(psd[idx_band], dx=freq_res)
    else:
        tp = simps(psd, dx=freq_res)

    output = {
        &#34;bandpower&#34;: bp,
        &#34;total_power&#34;: tp,
        &#34;relative_power&#34;: bp/tp}
    return output</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.bandpower_ratio"><code class="name flex">
<span>def <span class="ident">bandpower_ratio</span></span>(<span>self, first_band, second_band, win_sec, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculate the ratio in power between two bandpass filtered signals.</p>
<p>Note that common ranges are:
delta (1.5–4 Hz), theta (5-11 Hz)</p>
<h2 id="parameters">Parameters</h2>
<p>first_band - 1d array
lower and upper bands
second_band - 1d array
lower and upper bands
win_sec - float
length of the windows to bin lfp into in seconds.
recommend 4 for eg.
kwargs:
first_name - str name of band 1, default "Band 1"
second_name - str name of band 2, default "Band 2"</p>
<h2 id="returns">Returns</h2>
<p>float - the ratio between the power signals.</p>
<h2 id="see-also">See also</h2>
<p><code>nc_lfp.NLfp().bandpower()</code></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def bandpower_ratio(self, first_band, second_band, win_sec, **kwargs):
    &#34;&#34;&#34;
    Calculate the ratio in power between two bandpass filtered signals.

    Note that common ranges are:
    delta (1.5–4 Hz), theta (5-11 Hz)

    Parameters
    ----------
    first_band - 1d array
        lower and upper bands
    second_band - 1d array
        lower and upper bands
    win_sec - float
        length of the windows to bin lfp into in seconds.
        recommend 4 for eg.
    kwargs:
        first_name - str name of band 1, default &#34;Band 1&#34;
        second_name - str name of band 2, default &#34;Band 2&#34;

    Returns
    -------
    float - the ratio between the power signals.

    See also
    --------
    nc_lfp.NLfp().bandpower()
    &#34;&#34;&#34;

    _results = oDict()
    name1 = kwargs.get(&#34;first_name&#34;, &#34;Band 1&#34;)
    name2 = kwargs.get(&#34;second_name&#34;, &#34;Band 2&#34;)
    if &#34;window_sec&#34; not in kwargs:
        kwargs[&#34;window_sec&#34;] = win_sec

    b1 = self.bandpower(first_band, **kwargs)
    b2 = self.bandpower(second_band, **kwargs)
    if b1[&#34;total_power&#34;] != b2[&#34;total_power&#34;]:
        logging.error(
            &#34;Differing total power in lfp bandpower ratio calculations&#34;)
    bp = b1[&#34;bandpower&#34;] / b2[&#34;bandpower&#34;]
    key1 = name1 + &#34; Power&#34;
    key2 = name2 + &#34; Power&#34;
    key3 = name1 + &#34; &#34; + name2 + &#34; Power Ratio&#34;
    _results[key1] = b1[&#34;bandpower&#34;]
    _results[key1 + &#34; (Relative)&#34;] = b1[&#34;relative_power&#34;]
    _results[key2] = b2[&#34;bandpower&#34;]
    _results[key2 + &#34; (Relative)&#34;] = b2[&#34;relative_power&#34;]
    _results[key3] = bp
    _results[&#34;Total Power&#34;] = b1[&#34;total_power&#34;]
    self.update_result(_results)
    return bp</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.event_trig_average"><code class="name flex">
<span>def <span class="ident">event_trig_average</span></span>(<span>self, event_stamp=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Averaging event-triggered LFP signals</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>event_stamp</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the events or the spiking activities for measuring the
event triggered average of the LFP signal</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keywrod arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def event_trig_average(self, event_stamp=None, **kwargs):
    &#34;&#34;&#34;
    Averaging event-triggered LFP signals

    Parameters
    ----------
    event_stamp : ndarray
        Timestamps of the events or the spiking activities for measuring the
        event triggered average of the LFP signal
    **kwargs
        Keywrod arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;

    graph_data = oDict()
    window = np.array(kwargs.get(&#39;window&#39;, [-0.5, 0.5]))

    if event_stamp is None:
        spike = kwargs.get(&#39;spike&#39;, None)

        try:
            data_type = spike.get_type()
        except:
            logging.error(&#39;The data type of the addes object cannot be determined!&#39;)

        if data_type == &#39;spike&#39;:
            event_stamp = spike.get_unit_stamp()
        elif spike in self.get_spike_names():
            event_stamp = self.get_spike(spike).get_unit_stamp()

    if event_stamp is None:
        logging.error(&#39;No valid event timestamp or spike is provided&#39;)
    else:
        lfp = self.get_samples()*1000
        Fs = self.get_sampling_rate()
        time = self.get_timestamp()
        center = time.searchsorted(event_stamp, side=&#39;left&#39;)
        win = np.ceil(window*Fs).astype(int)
        win = np.arange(win[0], win[1])

        # Keep windows within data
        center = np.array([center[i] for i in range(0, len(event_stamp)) \
            if center[i]+ win[0] &gt;= 0 and center[i]+ win[-1] &lt;= time.size])

        eta = reduce(lambda y, x: y+ lfp[x+ win], center)
        eta = eta/center.size

        graph_data[&#39;t&#39;] = win/Fs
        graph_data[&#39;ETA&#39;] = eta
        graph_data[&#39;center&#39;] = center

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_bytes_per_sample"><code class="name flex">
<span>def <span class="ident">get_bytes_per_sample</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the number of bytes to represent each LFP waveform sample</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of bytes to represent each sample of the LFP waveform</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_bytes_per_sample(self):
    &#34;&#34;&#34;
    Returns the number of bytes to represent each LFP waveform sample

    Parameters
    ----------
    None

    Returns
    -------
    int
        Number of bytes to represent each sample of the LFP waveform

    &#34;&#34;&#34;

    return self._record_info[&#39;Bytes per sample&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_channel_id"><code class="name flex">
<span>def <span class="ident">get_channel_id</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the electrode channels ID</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>LFP channel ID</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_channel_id(self):
    &#34;&#34;&#34;
    Returns the electrode channels ID

    Parameters
    ----------
    None

    Returns
    -------
    str
        LFP channel ID

    &#34;&#34;&#34;

    return self._channel_id</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_file_tag"><code class="name flex">
<span>def <span class="ident">get_file_tag</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the file tag or extension for the LFP dataset. For example, Axona recordings usually
have file tags like 'eeg' or 'eeg8' etc.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>File tag or extension for the LFP dataset</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_file_tag(self):
    &#34;&#34;&#34;
    Returns the file tag or extension for the LFP dataset. For example, Axona recordings usually
    have file tags like &#39;eeg&#39; or &#39;eeg8&#39; etc.

    Parameters
    ----------
    None

    Returns
    -------
    str
        File tag or extension for the LFP dataset
    &#34;&#34;&#34;

    return self._file_tag</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_fullscale_mv"><code class="name flex">
<span>def <span class="ident">get_fullscale_mv</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the fullscale value of the ADC in mV</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Fullscale ADC value in mV</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_fullscale_mv(self):
    &#34;&#34;&#34;
    Returns the fullscale value of the ADC in mV

    Parameters
    ----------
    None

    Returns
    -------
    int
        Fullscale ADC value in mV

    &#34;&#34;&#34;

    return self._record_info[&#39;ADC Fullscale mv&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_recording_time"><code class="name flex">
<span>def <span class="ident">get_recording_time</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the recording time in seconds</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Recording time in seconds</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_recording_time(self):
    &#34;&#34;&#34;
    Returns the recording time in seconds

    Parameters
    ----------
    None

    Returns
    -------
    int
        Recording time in seconds
    &#34;&#34;&#34;

    return self.get_total_samples() / (self.get_sampling_rate())</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_samples"><code class="name flex">
<span>def <span class="ident">get_samples</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns LFP waveform samples</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Samples of the LFP signal</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_samples(self):
    &#34;&#34;&#34;
    Returns LFP waveform samples

    Parameters
    ----------
    None

    Returns
    -------
    ndarray
        Samples of the LFP signal

    &#34;&#34;&#34;

    return self._samples</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_sampling_rate"><code class="name flex">
<span>def <span class="ident">get_sampling_rate</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the sampling rate of spike waveforms</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Sampling rate for spike waveforms</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_sampling_rate(self):
    &#34;&#34;&#34;
    Returns the sampling rate of spike waveforms

    Parameters
    ----------
    None

    Returns
    -------
    int
        Sampling rate for spike waveforms

    &#34;&#34;&#34;

    return self._record_info[&#39;Sampling rate&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_timestamp"><code class="name flex">
<span>def <span class="ident">get_timestamp</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the timestamps of the LFP waveform</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Timestamps of the LFP signal</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_timestamp(self):
    &#34;&#34;&#34;
    Returns the timestamps of the LFP waveform

    Parameters
    ----------
    None

    Returns
    -------
    ndarray
        Timestamps of the LFP signal

    &#34;&#34;&#34;

    return self._timestamp</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_timestamp_bytes"><code class="name flex">
<span>def <span class="ident">get_timestamp_bytes</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the number of bytes to represent each timestamp in the binary file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of bytes to represent timestamps</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_timestamp_bytes(self):
    &#34;&#34;&#34;
    Returns the number of bytes to represent each timestamp in the binary file

    Parameters
    ----------
    None

    Returns
    -------
    int
        Number of bytes to represent timestamps

    &#34;&#34;&#34;

    return self._record_info[&#39;Bytes per timestamp&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_total_channel"><code class="name flex">
<span>def <span class="ident">get_total_channel</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns total number of electrode channels in the LFP data file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Total number of electrode channels</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_total_channel(self):
    &#34;&#34;&#34;
    Returns total number of electrode channels in the LFP data file

    Parameters
    ----------
    None

    Returns
    -------
    int
        Total number of electrode channels
    &#34;&#34;&#34;

    return self._record_info[&#39;No of channels&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_total_samples"><code class="name flex">
<span>def <span class="ident">get_total_samples</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns total number of LFP samples</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>ndarray</code></dt>
<dd>Total number of LFP samples</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_total_samples(self):
    &#34;&#34;&#34;
    Returns total number of LFP samples

    Parameters
    ----------
    None

    Returns
    -------
    ndarray
        Total number of LFP samples

    &#34;&#34;&#34;
    return self._record_info[&#39;No of samples&#39;]</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.get_type"><code class="name flex">
<span>def <span class="ident">get_type</span></span>(<span>self)</span>
</code></dt>
<dd>
<section class="desc"><p>Returns the type of object. For NLfp, this is always <code>lfp</code> type</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def get_type(self):
    &#34;&#34;&#34;
    Returns the type of object. For NLfp, this is always `lfp` type

    Parameters
    ----------
    None

    Returns
    -------
    str

    &#34;&#34;&#34;
    return self.__type</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.load"><code class="name flex">
<span>def <span class="ident">load</span></span>(<span>self, filename=None, system=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads LFP datasets</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the spike datafile</dd>
<dt><strong><code>system</code></strong> :&ensp;<code>str</code></dt>
<dd>Recording system or format of the spike data file</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="see-also">See also</h2>
<p><code>load_lfp_axona()</code>, <code>load_lfp_NLX()</code>, <code>load_lfp_NWB()</code></p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load(self, filename=None, system=None):
    &#34;&#34;&#34;
    Loads LFP datasets

    Parameters
    ----------
    filename : str
        Name of the spike datafile
    system : str
        Recording system or format of the spike data file

    Returns
    -------
    None

    See also
    --------
    load_lfp_axona(), load_lfp_NLX(), load_lfp_NWB()

    &#34;&#34;&#34;
    if system is None:
        system = self._system
    else:
        self._system = system
    if filename is None:
        filename = self._filename
    else:
        self._filename = filename
    loader = getattr(self, &#39;load_lfp_&#39; + system)
    loader(filename)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.load_lfp"><code class="name flex">
<span>def <span class="ident">load_lfp</span></span>(<span>self, names=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Loads datasets of the LFP nodes. Name of each node is used for obtaining the
filenames</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>names</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Names of the nodes to load. If <code>all</code>, all LFP nodes are loaded</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_lfp(self, names=None):
    &#34;&#34;&#34;
    Loads datasets of the LFP nodes. Name of each node is used for obtaining the
    filenames

    Parameters
    ----------
    names : list of str
        Names of the nodes to load. If `all`, all LFP nodes are loaded

    Returns
    -------
    None
    &#34;&#34;&#34;

    if names is None:
        self.load()
    elif names == &#39;all&#39;:
        for lfp in self._lfp:
            lfp.load()
    else:
        logging.error(&#34;Lfp by name has yet to be implemented&#34;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.load_lfp_Axona"><code class="name flex">
<span>def <span class="ident">load_lfp_Axona</span></span>(<span>self, file_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes LFP data from Axona file format</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory for the lfp data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_lfp_Axona(self, file_name):
    &#34;&#34;&#34;
    Decodes LFP data from Axona file format

    Parameters
    ----------
    file_name : str
        Full file directory for the lfp data

    Returns
    -------
    None

    &#34;&#34;&#34;

    words = file_name.split(sep=os.sep)
    file_directory = os.sep.join(words[0:-1])
    file_tag, file_extension = words[-1].split(&#39;.&#39;)
    set_file = file_directory + os.sep + file_tag + &#39;.set&#39;

    self._set_data_source(file_name)
    self._set_source_format(&#39;Axona&#39;)

    if os.path.isfile(file_name):
        with open(file_name, &#39;rb&#39;) as f:
            while True:
                line = f.readline()
                try:
                    line = line.decode(&#39;latin-1&#39;)
                except:
                    break

                if line == &#39;&#39;:
                    break
                if line.startswith(&#39;trial_date&#39;):
                    self._set_date(&#39; &#39;.join(line.replace(&#39;,&#39;, &#39; &#39;).split()[1:]))
                if line.startswith(&#39;trial_time&#39;):
                    self._set_time(line.split()[1])
                if line.startswith(&#39;experimenter&#39;):
                    self._set_experiemnter(&#39; &#39;.join(line.split()[1:]))
                if line.startswith(&#39;comments&#39;):
                    self._set_comments(&#39; &#39;.join(line.split()[1:]))
                if line.startswith(&#39;duration&#39;):
                    self._set_duration(float(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;sw_version&#39;):
                    self._set_file_version(line.split()[1])
                if line.startswith(&#39;num_chans&#39;):
                    self._set_total_channel(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;sample_rate&#39;):
                    self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line))))
                if line.startswith(&#39;bytes_per_sample&#39;):
                    self._set_bytes_per_sample(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#39;num_&#39;+ file_extension[:3].upper() + &#39;_samples&#39;):
                    self._set_total_samples(int(&#39;&#39;.join(line.split()[1:])))
                if line.startswith(&#34;data_start&#34;):
                    break

            num_samples = self.get_total_samples()
            bytes_per_sample = self.get_bytes_per_sample()

            f.seek(0, 0)
            header_offset = []
            while True:
                try:
                    buff = f.read(10).decode(&#39;UTF-8&#39;)
                except:
                    break
                if buff == &#39;data_start&#39;:
                    header_offset = f.tell()
                    break
                else:
                    f.seek(-9, 1)

            eeg_ID = re.findall(r&#39;\d+&#39;, file_extension)
            self.set_file_tag(1 if not eeg_ID else int(eeg_ID[0]))
            max_ADC_count = 2**(8*bytes_per_sample-1)-1
            max_byte_value = 2**(8*bytes_per_sample)

            with open(set_file, &#39;r&#39;, encoding=&#39;latin-1&#39;) as f_set:
                lines = f_set.readlines()
                channel_lines = dict([tuple(map(int, re.findall(r&#39;\d+.\d+|\d+&#39;, line)[0].split()))\
                            for line in lines if line.startswith(&#39;EEG_ch_&#39;)])
                channel_id = channel_lines[self.get_file_tag()]
                self.set_channel_id(channel_id)

                gain_lines = dict([tuple(map(int, re.findall(r&#39;\d+.\d+|\d+&#39;, line)[0].split()))\
                        for line in lines if &#39;gain_ch_&#39; in line])
                gain = gain_lines[channel_id-1]

                for line in lines:
                    if line.startswith(&#39;ADC_fullscale_mv&#39;):
                        self._set_fullscale_mv(int(re.findall(r&#39;\d+.\d+|d+&#39;, line)[0]))
                        break
                AD_bit_uvolt = 2*self.get_fullscale_mv()/ \
                                (gain*np.power(2, 8*bytes_per_sample))

            record_size = bytes_per_sample
            sample_le = 256**(np.arange(0, bytes_per_sample, 1))

            if not header_offset:
                print(&#39;Error: data_start marker not found!&#39;)
            else:
                f.seek(header_offset, 0)
                byte_buffer = np.fromfile(f, dtype=&#39;uint8&#39;)
                len_bytebuffer = len(byte_buffer)
                end_offset = len(&#39;\r\ndata_end\r&#39;)
                lfp_wave = np.zeros([num_samples, ], dtype=np.float64)
                for k in np.arange(0, bytes_per_sample, 1):
                    byte_offset = k
                    sample_value = (sample_le[k]* byte_buffer[byte_offset \
                                :byte_offset+ len_bytebuffer- end_offset- record_size\
                                :record_size])
                    if sample_value.size &lt; num_samples:
                        sample_value = np.append(sample_value, np.zeros([num_samples-sample_value.size,]))
                    sample_value = sample_value.astype(np.float64, casting=&#39;unsafe&#39;, copy=False)
                    np.add(lfp_wave, sample_value, out=lfp_wave)
                np.putmask(lfp_wave, lfp_wave &gt; max_ADC_count, lfp_wave- max_byte_value)

                self._set_samples(lfp_wave*AD_bit_uvolt)
                self._set_timestamp(np.arange(0, num_samples, 1)/self.get_sampling_rate())

    else:
        logging.error(
            &#34;No lfp file found for file {}&#34;.format(file_name))</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.load_lfp_NWB"><code class="name flex">
<span>def <span class="ident">load_lfp_NWB</span></span>(<span>self, file_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes LFP data from NWB (HDF5) file format</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory for the lfp data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_lfp_NWB(self, file_name):
    &#34;&#34;&#34;
    Decodes LFP data from NWB (HDF5) file format

    Parameters
    ----------
    file_name : str
        Full file directory for the lfp data

    Returns
    -------
    None

    &#34;&#34;&#34;

    file_name, path = file_name.split(&#39;+&#39;)
    if os.path.exists(file_name):
        hdf = Nhdf()
        hdf.set_filename(file_name)

        _record_info = {}

        if path in hdf.f:
            g = hdf.f[path]
        elif &#39;/processing/Neural Continuous/LFP/&#39;+ path in hdf.f:
            path = &#39;/processing/Neural Continuous/LFP/&#39;+ path
            g = hdf.f[path]
        else:
            logging.error(&#39;Specified path does not exist!&#39;)

        for key, value in g.attrs.items():
            _record_info[key] = value

        self.set_record_info(_record_info)

        self._set_samples(hdf.get_dataset(group=g, name=&#39;data&#39;))
        self._set_timestamp(hdf.get_dataset(group=g, name=&#39;timestamps&#39;))
        self._set_total_samples(hdf.get_dataset(group=g, name=&#39;num_samples&#39;))

        hdf.close()
    else:
        logging.error(file_name + &#39; does not exist!&#39;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.load_lfp_Neuralynx"><code class="name flex">
<span>def <span class="ident">load_lfp_Neuralynx</span></span>(<span>self, file_name)</span>
</code></dt>
<dd>
<section class="desc"><p>Decodes LFP data from Neuralynx file format</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory for the lfp data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_lfp_Neuralynx(self, file_name):
    &#34;&#34;&#34;
    Decodes LFP data from Neuralynx file format

    Parameters
    ----------
    file_name : str
        Full file directory for the lfp data

    Returns
    -------
    None

    &#34;&#34;&#34;

    self._set_data_source(file_name)
    self._set_source_format(&#39;Neuralynx&#39;)

    # Format description for the NLX file:

    resamp_freq = 250 # NeuroChaT subsamples the original recording from 32000 to 250

    header_offset = 16*1024 # fixed for NLX files

    bytes_per_timestamp = 8
    bytes_chan_no = 4
    bytes_sample_freq = 4
    bytes_num_valid_samples = 4
    bytes_per_sample = 2
    samples_per_record = 512

    max_byte_value = np.power(2, bytes_per_sample*8)
    max_ADC_count = np.power(2, bytes_per_sample*8- 1)-1
    AD_bit_uvolt = 10**-6

    self._set_bytes_per_sample(bytes_per_sample)

    record_size = None
    with open(file_name, &#39;rb&#39;) as f:
        while True:
            line = f.readline()
            try:
                line = line.decode(&#39;UTF-8&#39;)
            except:
                break

            if line == &#39;&#39;:
                break
            if &#39;SamplingFrequency&#39; in line:
                self._set_sampling_rate(float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))) # We are subsampling from the blocks of 512 samples per record
            if &#39;RecordSize&#39; in line:
                record_size = int(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
            if &#39;Time Opened&#39; in line:
                self._set_date(re.search(r&#39;\d+/\d+/\d+&#39;, line).group())
                self._set_time(re.search(r&#39;\d+:\d+:\d+&#39;, line).group())
            if &#39;FileVersion&#39; in line:
                self._set_file_version(line.split()[1])
            if &#39;ADMaxValue&#39; in line:
                max_ADC_count = float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))
            if &#39;ADBitVolts&#39; in line:
                AD_bit_uvolt = float(&#39;&#39;.join(re.findall(r&#39;\d+.\d+|\d+&#39;, line)))*(10**6)

        self._set_fullscale_mv(max_byte_value*AD_bit_uvolt/2) # gain = 1 assumed to keep in similarity to Axona

        if not record_size:
            record_size = bytes_per_timestamp+ \
                         bytes_chan_no+ \
                         bytes_sample_freq+ \
                         bytes_num_valid_samples+ \
                         bytes_per_sample*samples_per_record

        time_offset = 0
        sample_freq_offset = bytes_per_timestamp+ bytes_chan_no
        num_valid_samples_offset = sample_freq_offset+ bytes_sample_freq
        sample_offset = num_valid_samples_offset+ bytes_num_valid_samples
        f.seek(0, 2)
        num_samples = int((f.tell()- header_offset)/record_size)

        f.seek(header_offset, 0)
        time = np.array([])
        lfp_wave = np.array([])
        sample_le = 256**(np.arange(0, bytes_per_sample, 1))
        for _ in np.arange(num_samples):
            sample_bytes = np.fromfile(f, dtype=&#39;uint8&#39;, count=record_size)
            block_start = int.from_bytes(sample_bytes[time_offset+ \
                            np.arange(bytes_per_timestamp)], byteorder=&#39;little&#39;, signed=False)/10**6
            valid_samples = int.from_bytes(sample_bytes[num_valid_samples_offset+ \
                            np.arange(bytes_num_valid_samples)], byteorder=&#39;little&#39;, signed=False)
            sampling_freq = int.from_bytes(sample_bytes[sample_freq_offset+ \
                            np.arange(bytes_sample_freq)], byteorder=&#39;little&#39;, signed=False)

            wave_bytes = sample_bytes[sample_offset+ np.arange(valid_samples* bytes_per_sample)]\
                            .reshape([valid_samples, bytes_per_sample])
            block_wave = np.dot(wave_bytes, sample_le)
            #    for k in np.arange(valid_samples):
            #        block_wave[k] = int.from_bytes(sample_bytes[sample_offset+ k*bytes_per_sample+ \
            #                    np.arange(bytes_per_sample)], byteorder=&#39;little&#39;, signed=False)
            np.putmask(block_wave, block_wave &gt; max_ADC_count, block_wave - max_byte_value)
            block_wave = block_wave*AD_bit_uvolt
            block_time = block_start +  np.arange(valid_samples)/ sampling_freq
            interp_time = np.arange(block_start, block_time[-1], 1/resamp_freq)
            interp_wave = np.interp(interp_time, block_time, block_wave)
            time = np.append(time, interp_time)
            lfp_wave = np.append(lfp_wave, interp_wave)
        time -= time.min()
        self._set_samples(lfp_wave)
        self._set_total_samples(lfp_wave.size)
        self._set_timestamp(time)
        self._set_sampling_rate(resamp_freq)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.load_spike"><code class="name flex">
<span>def <span class="ident">load_spike</span></span>(<span>self, names='all')</span>
</code></dt>
<dd>
<section class="desc"><p>Loads datasets of the spike nodes. Name of each node is used for obtaining the
filenames</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>names</code></strong> :&ensp;<code>list</code> of <code>str</code></dt>
<dd>Names of the nodes to load. If None, current NSpike() object is loaded</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def load_spike(self, names=&#39;all&#39;):
    &#34;&#34;&#34;
    Loads datasets of the spike nodes. Name of each node is used for obtaining the
    filenames

    Parameters
    ----------
    names : list of str
        Names of the nodes to load. If None, current NSpike() object is loaded

    Returns
    -------
    None

    &#34;&#34;&#34;


    if names == &#39;all&#39;:
        for spike in self._spikes:
            spike.load()
    else:
        logging.error(&#34;Spikes by name has yet to be implemented&#34;)</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.phase_at_events"><code class="name flex">
<span>def <span class="ident">phase_at_events</span></span>(<span>self, event_stamps, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Phase based on times.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>event_stamps</code></strong> :&ensp;<code>array</code></dt>
<dd>an array of event times</dd>
</dl>
<p>**kwargs:
keyword arguments</p>
<h2 id="returns">Returns</h2>
<pre><code>(array)
Phase values for each position
</code></pre></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def phase_at_events(self, event_stamps, **kwargs):
    &#34;&#34;&#34;
    Phase based on times.

    Parameters
    ----------
    event_stamps : array
        an array of event times
    **kwargs:
        keyword arguments

    Returns
    -------
        (array)
        Phase values for each position
    &#34;&#34;&#34;
    lfp = self.get_samples() * 1000
    Fs = self.get_sampling_rate()
    time = self.get_timestamp()

    # Input parameters
    fwin = kwargs.get(&#39;fwin&#39;, [6, 12])

    # Filter
    fmax = fwin[1]
    fmin = fwin[0]
    _filter = [5, fmin, fmax, &#39;bandpass&#39;]
    _prefilt = kwargs.get(&#39;filtset&#39;, [10, 1.5, 40, &#39;bandpass&#39;])

    b_lfp = butter_filter(lfp, Fs, *_filter)  # band LFP
    lfp = butter_filter(lfp, Fs, *_prefilt)

    # Measure phase
    hilb = sg.hilbert(b_lfp)
    phase = np.angle(hilb, deg=True)
    phase[phase &lt; 0] = phase[phase &lt; 0] + 360

    ephase = np.interp(event_stamps, time, phase)

    return ephase</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.phase_dist"><code class="name flex">
<span>def <span class="ident">phase_dist</span></span>(<span>self, event_stamp, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analysis of spike to LFP phase distribution</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>evnet_stamp</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the events of spiking activities for measring the phase
distribution</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keywrod arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">    def phase_dist(self, event_stamp, **kwargs):
        &#34;&#34;&#34;
        Analysis of spike to LFP phase distribution

        Parameters
        ----------
        evnet_stamp : ndarray
            Timestamps of the events of spiking activities for measring the phase
            distribution
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        _results= oDict()
        graph_data = oDict()

        cs = CircStat()

        lfp = self.get_samples()*1000
        Fs = self.get_sampling_rate()
        time = self.get_timestamp()

        # Input parameters
        bins = int(360/kwargs.get(&#39;binsize&#39;, 5))
        rbinsize = kwargs.get(&#39;rbinsize&#39;, 2) # raster binsize
        rbins = int(360/rbinsize)
        fwin = kwargs.get(&#39;fwin&#39;, [6, 12])
        pratio = kwargs.get(&#39;pratio&#39;, 0.2)
        aratio = kwargs.get(&#39;aratio&#39;, 0.15)

    # Filter
        fmax = fwin[1]
        fmin = fwin[0]
        _filter = [5, fmin, fmax, &#39;bandpass&#39;]
        _prefilt = kwargs.get(&#39;filtset&#39;, [10, 1.5, 40, &#39;bandpass&#39;])

        b_lfp = butter_filter(lfp, Fs, *_filter) # band LFP
        lfp = butter_filter(lfp, Fs, *_prefilt)

    # Measure phase
        hilb = sg.hilbert(b_lfp)
#        self.hilb = hilb
#        phase = np.remainder(np.angle(hilb, deg=True)+ 360, 360)
        phase = np.angle(hilb, deg=True)
        phase[phase &lt; 0] = phase[phase &lt; 0] + 360
        mag = np.abs(hilb)

        ephase = np.interp(event_stamp, time, phase)

        p2p = np.abs(np.max(lfp) - np.min(lfp))
        xline = 0.5* np.mean(mag) # cross line

        # Detection algo
        # zero cross
        mag1 = mag[0:-3]
        mag2 = mag[1:-2]
        mag3 = mag[2:-1]

        xind = np.union1d(find(np.logical_and(mag1 &lt; xline, mag2 &gt; xline)), \
                find(np.logical_and(np.logical_and(mag1 &lt; xline, mag2 == xline), mag3 &gt; xline)))

        # Ignore segments &lt;1/fmax
        i = 0
        rcount = np.empty([0,])
        bcount = np.empty([0, 0])

        phBins = np.arange(0, 360, 360/bins)
        rbins = np.arange(0, 360, 360/rbins)

        seg_count = 0
        while i &lt; len(xind)-1:
            k = i+1
            while time[xind[k]]- time[xind[i]] &lt; 1/fmin and k &lt; len(xind)-1:
                k += 1
#            print(time[xind[i]], time[xind[k]])
            s_lfp = lfp[xind[i]: xind[k]]
            s_p2p = np.abs(np.max(s_lfp)- np.min(s_lfp))

            if s_p2p &gt;= aratio*p2p:
                s_psd, f = fft_psd(s_lfp, Fs)
                if np.sum(s_psd[np.logical_and(f &gt;= fmin, f &lt;= fmax)]) &gt; pratio* np.sum(s_psd):
                    # Phase distribution
                    s_phase = ephase[np.logical_and(event_stamp &gt; time[xind[i]], event_stamp &lt;= time[xind[k]])]
#                    print(s_phase.shape, s_phase.shape)

                    if not s_phase.shape[0]:
                        pass
                    else:
                        seg_count += 1
                        cs.set_theta(s_phase)
                        temp_count = cs.circ_histogram(bins=rbinsize)
#                        temp_count = np.histogram(s_phase, bins=rbins, range=[0, 360])
                        temp_count = temp_count[0]
                        if not rcount.size:
                            rcount = temp_count
                        else:
                            rcount = np.append(rcount, temp_count)

                        temp_count = np.histogram(s_phase, bins=bins, range=[0, 360])
                        temp_count = np.resize(temp_count[0], [1, bins])
                        if not len(bcount):
                            bcount = temp_count
                        else:
                            bcount = np.append(bcount, temp_count, axis=0)
            i = k

        rcount = rcount.reshape([seg_count, rbins.size])

        phCount = np.sum(bcount, axis=0)

        cs.set_rho(phCount)
        cs.set_theta(phBins)

        cs.calc_stat()
        result = cs.get_result()
        meanTheta = result[&#39;meanTheta&#39;]*np.pi/180

        _results[&#39;LFP Spike Mean Phase&#39;]= result[&#39;meanTheta&#39;]
        _results[&#39;LFP Spike Mean Phase Count&#39;]= result[&#39;meanRho&#39;]
        _results[&#39;LFP Spike Phase Res Vect&#39;]= result[&#39;resultant&#39;]

        graph_data[&#39;meanTheta&#39;] = meanTheta
        graph_data[&#39;phCount&#39;] = phCount
        graph_data[&#39;phBins&#39;] = phBins
        graph_data[&#39;raster&#39;] = rcount
        graph_data[&#39;rasterbins&#39;] = rbins

        self.update_result(_results)

        return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.plv"><code class="name flex">
<span>def <span class="ident">plv</span></span>(<span>self, event_stamp, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Calculates phase-locking value of the spike train to underlying LFP signal.</p>
<p>When 'mode'= None in the inpput kwargs, it calculates the PLV and SFC over
the entire spike-train.</p>
<p>If 'mode'= 'bs', it bootstraps the spike-timestamps
and calculates the locking values for each set of new spike timestamps.</p>
<p>If 'mode'= 'tr', a time-resilved phase-locking analysis is performed where
the LFP signal is split into overlapped segments for each calculation.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>evnet_stamp</code></strong> :&ensp;<code>ndarray</code></dt>
<dd>Timestamps of the events or the spiking activities for measuring the phase
locking</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keywrod arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def plv(self, event_stamp, **kwargs):
    &#34;&#34;&#34;
    Calculates phase-locking value of the spike train to underlying LFP signal.

    When &#39;mode&#39;= None in the inpput kwargs, it calculates the PLV and SFC over
    the entire spike-train.

    If &#39;mode&#39;= &#39;bs&#39;, it bootstraps the spike-timestamps
    and calculates the locking values for each set of new spike timestamps.

    If &#39;mode&#39;= &#39;tr&#39;, a time-resilved phase-locking analysis is performed where
    the LFP signal is split into overlapped segments for each calculation.

    Parameters
    ----------
    evnet_stamp : ndarray
        Timestamps of the events or the spiking activities for measuring the phase
        locking
    **kwargs
        Keywrod arguments

    Returns
    -------
    dict
        Graphical data of the analysis

    &#34;&#34;&#34;
    graph_data = oDict()

    lfp = self.get_samples()*1000
    Fs = self.get_sampling_rate()
    time = self.get_timestamp()

    window = np.array(kwargs.get(&#39;window&#39;, [-0.5, 0.5]))
    win = np.ceil(window*Fs).astype(int)
    win = np.arange(win[0], win[1])
    slep_win = sg.hann(win.size, False)

    nfft = kwargs.get(&#39;nfft&#39;, 1024)
    mode = kwargs.get(&#39;mode&#39;, None) # None, &#39;bs&#39;, &#39;tr&#39; bs=bootstrp, tr=time-resolved
    fwin = kwargs.get(&#39;fwin&#39;, [])

    xf = np.arange(0, Fs, Fs/nfft)
    f = xf[0: int(nfft/2)+ 1]

    ind = np.arange(f.size) if len(fwin) == 0 else find(np.logical_and(f &gt;= fwin[0], f &lt;= fwin[1]))

    if mode == &#39;bs&#39;:
        nsample = kwargs.get(&#39;nsample&#39;, 50)
        nrep = kwargs.get(&#39;nrep&#39;, 500)

        STA = np.empty([nrep, win.size])
        fSTA = np.empty([nrep, ind.size])
        STP = np.empty([nrep, ind.size])
        SFC = np.empty([nrep, ind.size])
        PLV = np.empty([nrep, ind.size])

        for i in np.arange(nrep):
            data = self.plv(np.random.choice(event_stamp, nsample, False), \
                    window=window, nfft=nfft, mode=None, fwin=fwin)
            t = data[&#39;t&#39;]
            STA[i, :] = data[&#39;STA&#39;]
            fSTA[i, :] = data[&#39;fSTA&#39;]
            STP[i, :] = data[&#39;STP&#39;]
            SFC[i, :] = data[&#39;SFC&#39;]
            PLV[i, :] = data[&#39;PLV&#39;]

        graph_data[&#39;t&#39;] = t
        graph_data[&#39;f&#39;] = f[ind]
        graph_data[&#39;STAm&#39;] = STA.mean(0)
        graph_data[&#39;fSTAm&#39;] = fSTA.mean(0)
        graph_data[&#39;STPm&#39;] = STP.mean(0)
        graph_data[&#39;SFCm&#39;] = SFC.mean(0)
        graph_data[&#39;PLVm&#39;] = PLV.mean(0)

        graph_data[&#39;STAe&#39;] = stats.sem(STA, 0)
        graph_data[&#39;fSTAe&#39;] = stats.sem(fSTA, 0)
        graph_data[&#39;STPe&#39;] = stats.sem(STP, 0)
        graph_data[&#39;SFCe&#39;] = stats.sem(SFC, 0)
        graph_data[&#39;PLVe&#39;] = stats.sem(PLV, 0)

    elif mode == &#39;tr&#39;:
        nsample = kwargs.get(&#39;nsample&#39;, None)

        slide = kwargs.get(&#39;slide&#39;, 25) # in ms
        slide = slide/1000 # convert to sec

        offset = np.arange(window[0], window[-1], slide)
        nwin = offset.size

        fSTA = np.empty([nwin, ind.size])
        STP = np.empty([nwin, ind.size])
        SFC = np.empty([nwin, ind.size])
        PLV = np.empty([nwin, ind.size])

        if nsample is None or nsample &gt; event_stamp.size:
            stamp = event_stamp
        else:
            stamp = np.random.choice(event_stamp, nsample, False)

        for i in np.arange(nwin):
            data = self.plv(stamp + offset[i], \
                    nfft=nfft, mode=None, fwin=fwin, window=window)
            t = data[&#39;t&#39;]
            fSTA[i, :] = data[&#39;fSTA&#39;]
            STP[i, :] = data[&#39;STP&#39;]
            SFC[i, :] = data[&#39;SFC&#39;]
            PLV[i, :] = data[&#39;PLV&#39;]

        graph_data[&#39;offset&#39;] = offset
        graph_data[&#39;f&#39;] = f[ind]
        graph_data[&#39;fSTA&#39;] = fSTA.transpose()
        graph_data[&#39;STP&#39;] = STP.transpose()
        graph_data[&#39;SFC&#39;] = SFC.transpose()
        graph_data[&#39;PLV&#39;] = PLV.transpose()

    elif mode is None:
        center = time.searchsorted(event_stamp)
        # Keep windows within data
        center = np.array([center[i] for i in range(0, len(event_stamp)) \
            if center[i] + win[0] &gt;= 0 and center[i] + win[-1] &lt;= time.size])

        sta_data = self.event_trig_average(event_stamp, **kwargs)
        STA = sta_data[&#39;ETA&#39;]

        fSTA = fft(np.multiply(STA, slep_win), nfft)

        fSTA = np.absolute(fSTA[0: int(nfft/2)+ 1])**2/nfft**2
        fSTA[1:-1] = 2*fSTA[1:-1]

        fLFP = np.array([fft(np.multiply(lfp[x+ win], slep_win), nfft) \
                for x in center])

        STP = np.absolute(fLFP[:, 0: int(nfft/2)+ 1])**2/nfft**2
        STP[:, 1:-1] = 2*STP[:, 1:-1]
        STP = STP.mean(0)

        SFC = np.divide(fSTA, STP)*100

        PLV = np.copy(fLFP)

        # Normalize
        PLV = np.divide(PLV, np.absolute(PLV))
        PLV[np.isnan(PLV)] = 0

        PLV = np.absolute(PLV.mean(0))[0: int(nfft/2)+ 1]
        PLV[1:-1] = 2*PLV[1:-1]

        graph_data[&#39;t&#39;] = sta_data[&#39;t&#39;]
        graph_data[&#39;f&#39;] = f[ind]
        graph_data[&#39;STA&#39;] = STA
        graph_data[&#39;fSTA&#39;] = fSTA[ind]
        graph_data[&#39;STP&#39;] = STP[ind]
        graph_data[&#39;SFC&#39;] = SFC[ind]
        graph_data[&#39;PLV&#39;] = PLV[ind]

    return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.save_to_hdf5"><code class="name flex">
<span>def <span class="ident">save_to_hdf5</span></span>(<span>self, file_name=None, system=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Stores NLfp() object to HDF5 file</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_name</code></strong> :&ensp;<code>str</code></dt>
<dd>Full file directory for the lfp data</dd>
<dt><strong><code>system</code></strong> :&ensp;<code>str</code></dt>
<dd>Recoring system or data format</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl>
<h2 id="also-see">Also see</h2>
<p>nc_hdf.Nhdf().save_lfp()</p></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def save_to_hdf5(self, file_name=None, system=None):
    &#34;&#34;&#34;
    Stores NLfp() object to HDF5 file

    Parameters
    ----------
    file_name : str
        Full file directory for the lfp data
    system : str
        Recoring system or data format

    Returns
    -------
    None

    Also see
    --------
    nc_hdf.Nhdf().save_lfp()

    &#34;&#34;&#34;

    hdf = Nhdf()
    if file_name and system:
        if os.path.exists(file_name):
            self.set_filename(file_name)
            self.set_system(system)
            self.load()
        else:
            logging.error(&#39;Specified file cannot be found!&#39;)

    hdf.save_lfp(lfp=self)
    hdf.close()</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.set_channel_id"><code class="name flex">
<span>def <span class="ident">set_channel_id</span></span>(<span>self, channel_id='')</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the electrode channels ID</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>channel_id</code></strong> :&ensp;<code>str</code></dt>
<dd>Channel ID for the LFP data</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_channel_id(self, channel_id=&#39;&#39;):
    &#34;&#34;&#34;
    Sets the electrode channels ID

    Parameters
    ----------
    channel_id : str
        Channel ID for the LFP data

    Returns
    -------
    None

    &#34;&#34;&#34;
    self._channel_id = channel_id</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.set_file_tag"><code class="name flex">
<span>def <span class="ident">set_file_tag</span></span>(<span>self, file_tag)</span>
</code></dt>
<dd>
<section class="desc"><p>Sets the file tag or extension for the LFP dataset. For example, Axona recordings usually
have file tags like 'eeg' or 'eeg8' etc.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>file_tag</code></strong> :&ensp;<code>str</code></dt>
<dd>File tag or extension for the LFP dataset</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def set_file_tag(self, file_tag):
    &#34;&#34;&#34;
    Sets the file tag or extension for the LFP dataset. For example, Axona recordings usually
    have file tags like &#39;eeg&#39; or &#39;eeg8&#39; etc.

    Parameters
    ----------
    file_tag : str
        File tag or extension for the LFP dataset

    Returns
    -------
    None

    &#34;&#34;&#34;

    self._file_tag = file_tag</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.sharp_wave_ripples"><code class="name flex">
<span>def <span class="ident">sharp_wave_ripples</span></span>(<span>self, in_range=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Detect SWR events in the lfp, optionally in a given range</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>in_range</code></strong> :&ensp;<code>tuple</code></dt>
<dd>A range in seconds</dd>
</dl>
<h2 id="kwargs">kwargs</h2>
<dl>
<dt><strong><code>swr_lower</code></strong> :&ensp;<code>float</code></dt>
<dd>Lower band in hz</dd>
<dt><strong><code>swr_upper</code></strong> :&ensp;<code>float</code></dt>
<dd>Upper band in hz</dd>
<dt><strong><code>rms_window_size_ms</code></strong> :&ensp;<code>int</code></dt>
<dd>Size of the rms window in ms</dd>
<dt><strong><code>percentile</code></strong> :&ensp;<code>float</code></dt>
<dd>The percentile threshold for a peak</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>lfp times, lfp samples, swr times, lfp sample rate</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def sharp_wave_ripples(self, in_range=None, **kwargs):
    &#34;&#34;&#34;
    Detect SWR events in the lfp, optionally in a given range

    Parameters
    ----------
    in_range : tuple
        A range in seconds

    kwargs
    ------
    swr_lower : float
        Lower band in hz
    swr_upper : float
        Upper band in hz
    rms_window_size_ms : int
        Size of the rms window in ms
    percentile : float
        The percentile threshold for a peak

    Returns
    -------
    dict
        lfp times, lfp samples, swr times, lfp sample rate

    &#34;&#34;&#34;
    swr_lower = kwargs.get(&#34;swr_lower&#34;, 100)
    swr_higher = kwargs.get(&#34;swr_upper&#34;, 250)
    rms_window_size_ms = kwargs.get(&#34;rms_window_size_ms&#34;, 7)
    percentile = kwargs.get(&#34;peak_percentile&#34;, 99.5)

    lfp = self.subsample(in_range)
    sample_rate = lfp.get_sampling_rate()
    # Estimate SWR events
    filtered_lfp = butter_filter(
        lfp.get_samples(), sample_rate, 10,
        swr_lower, swr_higher, &#39;bandpass&#39;)
    rms_window_size = floor((rms_window_size_ms / 1000) * sample_rate)
    rms_envelope = window_rms(filtered_lfp, rms_window_size, mode=&#34;same&#34;)
    p_val = np.percentile(rms_envelope, percentile)
    _, peaks = find_peaks(rms_envelope, thresh=p_val)
    peaks = lfp.get_timestamp()[0] + (peaks / sample_rate)

    &#34;&#34;&#34;
    Alternative way to get SWR
    #rms_envelope = distinct_window_rms(filtered_lfp, rms_window_size)
    #peaks = (
    # longest_sleep_period[0] + peaks * rms_window_size) / sample_rate
    &#34;&#34;&#34;

    return {
        &#34;lfp times&#34;: lfp.get_timestamp(),
        &#34;lfp samples&#34;: filtered_lfp,
        &#34;swr times&#34;: peaks, &#34;lfp sample rate&#34;: sample_rate}</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.spectrum"><code class="name flex">
<span>def <span class="ident">spectrum</span></span>(<span>self, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>Analyses frequency spectrum of the LFP signal</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keywrod arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Graphical data of the analysis</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">    def spectrum(self, **kwargs):
        &#34;&#34;&#34;
        Analyses frequency spectrum of the LFP signal

        Parameters
        ----------
        **kwargs
            Keywrod arguments

        Returns
        -------
        dict
            Graphical data of the analysis

        &#34;&#34;&#34;

        graph_data = oDict()

        Fs = self.get_sampling_rate()
        slc = kwargs.get(&#39;slice&#39;, None)
        if slc:
            lfp = self.get_samples()[slc]
        else:
            lfp = self.get_samples()

        window = kwargs.get(&#39;window&#39;, 1.0)
        window = sg.get_window(&#39;hann&#39;, int(window*Fs)) if isinstance(window, float)\
                or isinstance(window, int) else window

        win_sec = np.ceil(window.size/Fs)

        noverlap = kwargs.get(&#39;noverlap&#39;, 0.5*win_sec)
        noverlap = noverlap if noverlap &lt; win_sec else 0.5*win_sec
        noverlap = np.ceil(noverlap*Fs)

        nfft = kwargs.get(&#39;nfft&#39;, 2*Fs)
        nfft = np.power(2, int(np.ceil(np.log2(nfft))))

        ptype = kwargs.get(&#39;ptype&#39;, &#39;psd&#39;)
        ptype = &#39;spectrum&#39; if ptype == &#39;power&#39; else &#39;density&#39;

        prefilt = kwargs.get(&#39;prefilt&#39;, True)
        _filter = kwargs.get(&#39;filtset&#39;, [10, 1.5, 40, &#39;bandpass&#39;])

        fmax = kwargs.get(&#39;fmax&#39;, Fs/2)

        if prefilt:
            lfp = butter_filter(lfp, Fs, *_filter)

        tr = kwargs.get(&#39;tr&#39;, False)
        db = kwargs.get(&#39;db&#39;, False)
        if tr:
            f, t, Sxx = sg.spectrogram(lfp, fs=Fs, \
                    window=window, nperseg=window.size, noverlap=noverlap, nfft=nfft, \
                    detrend=&#39;constant&#39;, return_onesided=True, scaling=ptype)

            graph_data[&#39;t&#39;] = t
            graph_data[&#39;f&#39;] = f[find(f &lt;= fmax)]

            if db:
                Sxx = 10*np.log10(Sxx/np.amax(Sxx))
                Sxx = Sxx.flatten()
                Sxx[find(Sxx &lt; -40)] = -40
                Sxx = np.reshape(Sxx, [f.size, t.size])

#            graph_data[&#39;Sxx&#39;] = np.empty([find(f&lt;= fmax).size, t.size])
#            graph_data[&#39;Sxx&#39;] = np.array([Sxx[i, :] for i in find(f&lt;= fmax)])
            graph_data[&#39;Sxx&#39;] = Sxx[find(f &lt;= fmax), :]
        else:
            f, Pxx = sg.welch(lfp, fs=Fs, \
                    window=window, nperseg=window.size, noverlap=noverlap, nfft=nfft, \
                    detrend=&#39;constant&#39;, return_onesided=True, scaling=ptype)

            graph_data[&#39;f&#39;] = f[find(f &lt;= fmax)]

            if db:
                Pxx = 10*np.log10(Pxx/Pxx.max())
                Pxx[find(Pxx &lt; -40)] = -40
            graph_data[&#39;Pxx&#39;] = Pxx[find(f &lt;= fmax)]

        return graph_data</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.spike_lfp_causality"><code class="name flex">
<span>def <span class="ident">spike_lfp_causality</span></span>(<span>self, spike=None, **kwargs)</span>
</code></dt>
<dd>
<section class="desc"><p>(Not implemented yet)</p>
<p>Analyses spike to underlying LFP causality</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>spike</code></strong> :&ensp;<code>NSpike</code></dt>
<dd>Spike dataset which is used for the causality analysis</dd>
<dt><strong><code>**kwargs</code></strong></dt>
<dd>Keywrod arguments</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>Should return graphical data of the analysis. The function is not
implemented yet.</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def spike_lfp_causality(self, spike=None, **kwargs):
    &#34;&#34;&#34;
    (Not implemented yet)

    Analyses spike to underlying LFP causality

    Parameters
    ----------
    spike : NSpike
        Spike dataset which is used for the causality analysis
    **kwargs
        Keywrod arguments

    Returns
    -------
    dict
        Should return graphical data of the analysis. The function is not
        implemented yet.

    &#34;&#34;&#34;

    pass</code></pre>
</details>
</dd>
<dt id="neurochat.nc_lfp.NLfp.subsample"><code class="name flex">
<span>def <span class="ident">subsample</span></span>(<span>self, sample_range=None)</span>
</code></dt>
<dd>
<section class="desc"><p>Extract a time range from the lfp.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>sample_range</code></strong> :&ensp;<code>tuple</code></dt>
<dd>the time in seconds to extract from the lfp</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><a title="neurochat.nc_lfp.NLfp" href="#neurochat.nc_lfp.NLfp"><code>NLfp</code></a></dt>
<dd>subsampled version of initial lfp object</dd>
</dl></section>
<details class="source">
<summary>Source code</summary>
<pre><code class="python">def subsample(self, sample_range=None):
    &#34;&#34;&#34;
    Extract a time range from the lfp.

    Parameters
    ----------
    sample_range : tuple
        the time in seconds to extract from the lfp

    Returns
    -------
    NLfp
        subsampled version of initial lfp object
    &#34;&#34;&#34;
    in_range = sample_range
    sample_rate = self.get_sampling_rate()
    if in_range is None:
        length = int(self.get_duration() * sample_rate)
        if (length != self.get_total_samples()):
            logging.warning(
                &#34;Unequal calculated and recorded total lfp samples&#34; +
                &#34;Calculated {} and recorded {}&#34;.format(
                    length, self.get_total_samples()))
        return self
    else:
        new_lfp = deepcopy(self)
        lfp_samples = self.get_samples()[
            int(sample_rate * in_range[0]):int(sample_rate * in_range[1])]
        lfp_times = self.get_timestamp()[
            int(sample_rate * in_range[0]):int(sample_rate * in_range[1])]
        new_lfp._set_samples(lfp_samples)
        new_lfp._set_timestamp(lfp_times)
        new_lfp._set_total_samples(len(lfp_samples))
        new_lfp._set_duration(in_range[1] - in_range[0])
        return new_lfp</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="neurochat.nc_base.NBase" href="nc_base.html#neurochat.nc_base.NBase">NBase</a></b></code>:
<ul class="hlist">
<li><code><a title="neurochat.nc_base.NBase.add_node" href="nc_base.html#neurochat.nc_base.NBase.add_node">add_node</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.change_names" href="nc_base.html#neurochat.nc_base.NBase.change_names">change_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.count_lfp" href="nc_base.html#neurochat.nc_base.NBase.count_lfp">count_lfp</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.count_spike" href="nc_base.html#neurochat.nc_base.NBase.count_spike">count_spike</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.del_lfp" href="nc_base.html#neurochat.nc_base.NBase.del_lfp">del_lfp</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.del_node" href="nc_base.html#neurochat.nc_base.NBase.del_node">del_node</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.del_spike" href="nc_base.html#neurochat.nc_base.NBase.del_spike">del_spike</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_comments" href="nc_base.html#neurochat.nc_base.NAbstract.get_comments">get_comments</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_data_source" href="nc_base.html#neurochat.nc_base.NAbstract.get_data_source">get_data_source</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_date" href="nc_base.html#neurochat.nc_base.NAbstract.get_date">get_date</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_duration" href="nc_base.html#neurochat.nc_base.NAbstract.get_duration">get_duration</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_experimenter" href="nc_base.html#neurochat.nc_base.NAbstract.get_experimenter">get_experimenter</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_file_version" href="nc_base.html#neurochat.nc_base.NAbstract.get_file_version">get_file_version</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_filename" href="nc_base.html#neurochat.nc_base.NAbstract.get_filename">get_filename</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_lfp" href="nc_base.html#neurochat.nc_base.NBase.get_lfp">get_lfp</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_lfp_names" href="nc_base.html#neurochat.nc_base.NBase.get_lfp_names">get_lfp_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_name" href="nc_base.html#neurochat.nc_base.NAbstract.get_name">get_name</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_node" href="nc_base.html#neurochat.nc_base.NBase.get_node">get_node</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_record_info" href="nc_base.html#neurochat.nc_base.NAbstract.get_record_info">get_record_info</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_results" href="nc_base.html#neurochat.nc_base.NAbstract.get_results">get_results</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_source_format" href="nc_base.html#neurochat.nc_base.NAbstract.get_source_format">get_source_format</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_spike" href="nc_base.html#neurochat.nc_base.NBase.get_spike">get_spike</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_spike_names" href="nc_base.html#neurochat.nc_base.NBase.get_spike_names">get_spike_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_system" href="nc_base.html#neurochat.nc_base.NAbstract.get_system">get_system</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.get_time" href="nc_base.html#neurochat.nc_base.NAbstract.get_time">get_time</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.reset_results" href="nc_base.html#neurochat.nc_base.NAbstract.reset_results">reset_results</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_description" href="nc_base.html#neurochat.nc_base.NAbstract.set_description">set_description</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_filename" href="nc_base.html#neurochat.nc_base.NAbstract.set_filename">set_filename</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_lfp_file_names" href="nc_base.html#neurochat.nc_base.NBase.set_lfp_file_names">set_lfp_file_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_lfp_names" href="nc_base.html#neurochat.nc_base.NBase.set_lfp_names">set_lfp_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_name" href="nc_base.html#neurochat.nc_base.NAbstract.set_name">set_name</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_node_file_names" href="nc_base.html#neurochat.nc_base.NBase.set_node_file_names">set_node_file_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_record_info" href="nc_base.html#neurochat.nc_base.NAbstract.set_record_info">set_record_info</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_spike_file_names" href="nc_base.html#neurochat.nc_base.NBase.set_spike_file_names">set_spike_file_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_spike_names" href="nc_base.html#neurochat.nc_base.NBase.set_spike_names">set_spike_names</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.set_system" href="nc_base.html#neurochat.nc_base.NAbstract.set_system">set_system</a></code></li>
<li><code><a title="neurochat.nc_base.NBase.update_result" href="nc_base.html#neurochat.nc_base.NAbstract.update_result">update_result</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="neurochat" href="index.html">neurochat</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="neurochat.nc_lfp.NLfp" href="#neurochat.nc_lfp.NLfp">NLfp</a></code></h4>
<ul class="">
<li><code><a title="neurochat.nc_lfp.NLfp.add_lfp" href="#neurochat.nc_lfp.NLfp.add_lfp">add_lfp</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.add_spike" href="#neurochat.nc_lfp.NLfp.add_spike">add_spike</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.bandpower" href="#neurochat.nc_lfp.NLfp.bandpower">bandpower</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.bandpower_ratio" href="#neurochat.nc_lfp.NLfp.bandpower_ratio">bandpower_ratio</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.event_trig_average" href="#neurochat.nc_lfp.NLfp.event_trig_average">event_trig_average</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_bytes_per_sample" href="#neurochat.nc_lfp.NLfp.get_bytes_per_sample">get_bytes_per_sample</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_channel_id" href="#neurochat.nc_lfp.NLfp.get_channel_id">get_channel_id</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_file_tag" href="#neurochat.nc_lfp.NLfp.get_file_tag">get_file_tag</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_fullscale_mv" href="#neurochat.nc_lfp.NLfp.get_fullscale_mv">get_fullscale_mv</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_recording_time" href="#neurochat.nc_lfp.NLfp.get_recording_time">get_recording_time</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_samples" href="#neurochat.nc_lfp.NLfp.get_samples">get_samples</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_sampling_rate" href="#neurochat.nc_lfp.NLfp.get_sampling_rate">get_sampling_rate</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_timestamp" href="#neurochat.nc_lfp.NLfp.get_timestamp">get_timestamp</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_timestamp_bytes" href="#neurochat.nc_lfp.NLfp.get_timestamp_bytes">get_timestamp_bytes</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_total_channel" href="#neurochat.nc_lfp.NLfp.get_total_channel">get_total_channel</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_total_samples" href="#neurochat.nc_lfp.NLfp.get_total_samples">get_total_samples</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.get_type" href="#neurochat.nc_lfp.NLfp.get_type">get_type</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.load" href="#neurochat.nc_lfp.NLfp.load">load</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.load_lfp" href="#neurochat.nc_lfp.NLfp.load_lfp">load_lfp</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.load_lfp_Axona" href="#neurochat.nc_lfp.NLfp.load_lfp_Axona">load_lfp_Axona</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.load_lfp_NWB" href="#neurochat.nc_lfp.NLfp.load_lfp_NWB">load_lfp_NWB</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.load_lfp_Neuralynx" href="#neurochat.nc_lfp.NLfp.load_lfp_Neuralynx">load_lfp_Neuralynx</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.load_spike" href="#neurochat.nc_lfp.NLfp.load_spike">load_spike</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.phase_at_events" href="#neurochat.nc_lfp.NLfp.phase_at_events">phase_at_events</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.phase_dist" href="#neurochat.nc_lfp.NLfp.phase_dist">phase_dist</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.plv" href="#neurochat.nc_lfp.NLfp.plv">plv</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.save_to_hdf5" href="#neurochat.nc_lfp.NLfp.save_to_hdf5">save_to_hdf5</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.set_channel_id" href="#neurochat.nc_lfp.NLfp.set_channel_id">set_channel_id</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.set_file_tag" href="#neurochat.nc_lfp.NLfp.set_file_tag">set_file_tag</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.sharp_wave_ripples" href="#neurochat.nc_lfp.NLfp.sharp_wave_ripples">sharp_wave_ripples</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.spectrum" href="#neurochat.nc_lfp.NLfp.spectrum">spectrum</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.spike_lfp_causality" href="#neurochat.nc_lfp.NLfp.spike_lfp_causality">spike_lfp_causality</a></code></li>
<li><code><a title="neurochat.nc_lfp.NLfp.subsample" href="#neurochat.nc_lfp.NLfp.subsample">subsample</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.6.3</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>